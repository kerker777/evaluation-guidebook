# 2025 年的模型評測：超越簡單基準，打造真正實用的模型

我相信我們應該以打造「運作良好」的模型為目標，而非追求「智慧」的模型——比起追求「通用智慧」來替我們解決問題（同時在過程中產生一大堆其他問題），更好的成功指標是打造出對人們有用且高效的工具。

根據 [Anthropic](https://www.anthropic.com/research/anthropic-economic-index-september-2025-report) 和 [OpenAI](https://cdn.openai.com/pdf/a253471f-8260-40c6-a2cc-aa93fe9f142e/economic-research-chatgpt-usage-paper.pdf) 最近的報告，大型語言模型當前最常見的使用場景確實是作為助理：用於編程、行政支援等等——而且模型在今年於通用智慧代理的使用案例上已經取得長足進步。

那麼，如果你想打造一個優秀的助理模型，應該測試什麼呢?

一個優秀的助理應該能夠做到以下幾點：接收到查詢後，它應該能處理指令中的模糊性、構建逐步計劃、正確識別必要資源、不偏離軌道地執行該計劃、根據需要呼叫工具、在執行計劃或發現新資訊時適應意外事件，而且全程不能胡說八道。這種行為需要多種能力的組合，例如逐步「推理」、長上下文記憶管理、適應性、低幻覺率，加上數學、程式碼和工具呼叫能力。小至 7B 的模型都可以成為優秀的智慧代理助理（儘管我們觀察到當規模降到 3B 以下會遇到瓶頸）。

我們如何評估智慧代理是否擅長所有這些任務？這需要多層次的方法：在開發過程中測試個別能力、衡量實際任務上的整合表現，以及探測在動態環境中的適應能力。

## 測試特定能力

你可以單獨評估**特定能力**——這在訓練時或比較基礎/預訓練模型時通常很有意義。（然而，如果你用以下評測來選擇和驗證訓練方法，那麼在最終模型上報告這些結果會稍微有偏差，因為你已經針對它們調整過訓練方法了）。

### 推理與常識

推理和常識資料集通常是「歷史性」的資料集，建立於 BERT 和嵌入模型的時代，也就是大型語言模型熱潮之前。它們在當時相當具有挑戰性（特別是因為它們通常是針對當時的模型進行對抗性構建的），但現在它們 1) 太容易了 2) 被污染/飽和了，應該只用於消融研究或預訓練評估。較大的資料集有時也包含錯誤或低品質問題，因為它們往往是透過 Amazon Mechanical Turk 建立的，以便快速且低成本地擴展（這就是現在使用大型語言模型生成評估問題的做法）。

[ARC]([https://arxiv.org/abs/1803.05457](https://arxiv.org/abs/1803.05457))（2018）（不要與 ARC-AGI 混淆）是一個從人類測試中建立的小學科學多選題資料集。這些選項是針對當時的詞共現系統進行對抗性選擇的。它有幾個子集，品質較高的 `challenge` 子集今天仍用於預訓練。[WinoGrande]([https://arxiv.org/pdf/1907.10641](https://arxiv.org/pdf/1907.10641))（2019）是一個眾包（mechanical turk + 驗證）的代詞消歧/填空資料集，使用對抗性項目對來欺騙模型。這兩個資料集對模型來說在 2022 到 2023 年之前都相當困難。

許多歷史資料集特別關注需要某種常識理解和基礎知識的推理。[HellaSwag]([https://arxiv.org/abs/1905.07830](https://arxiv.org/abs/1905.07830))（2019）要求大型語言模型在對抗性選項列表中選擇正確的下一句，其中文字來自 ActivityNet 的字幕和 Wikihow 的教學。（它是一個叫做 Swag 的資料集的後續版本）。由於大多數句子來自教學或活動描述，它們通常需要物理常識基礎知識才能解決。同樣地，[CommonsenseQA]([https://arxiv.org/abs/1811.00937](https://arxiv.org/abs/1811.00937))（2018）是一個從 ConceptNet 建立的常識多選題資料集——標註者撰寫問題，然後使用概念上相近的干擾項作為選項。[PIQA]([https://arxiv.org/abs/1911.11641](https://arxiv.org/abs/1911.11641))（2019）特別關注物理常識問題（從 [Instructables.com](http://Instructables.com) 的範例創建，同樣使用來自語義擾動或改寫的對抗性選項）。[OpenBookQA]([https://arxiv.org/abs/1809.02789](https://arxiv.org/abs/1809.02789))（2018）提供開卷事實來幫助回答多選題——然而，這些問題也需要潛在的常識知識。

### 知識
知識評估的主要資料集一直是 [MMLU](https://arxiv.org/abs/2009.03300)（2020）。它達到了飽和/污染狀態，經過更深入的檢查後，發現了許多問題：不完整的問題引用缺失的文件、不正確的標準答案、模糊的問題，以及在主題選擇上明顯的美國中心主義。因此，它在 [MMLU-Redux](https://arxiv.org/abs/2406.04127)（2024）中被清理，在 [**MMLU-Pro**](https://arxiv.org/abs/2406.01574)（2024，目前社群使用的主要替代品）中加入了更複雜的問題和更多答案，並在 [Global-MMLU](https://arxiv.org/abs/2412.03304)（2024）中被翻譯/標註文化偏見。這些主要用於預訓練評估和消融研究。

對於後訓練，人們關注更難的高品質知識資料集。[**GPQA**](https://arxiv.org/abs/2311.12022)（2023），客製化的博士級生物/化學/物理問題，旨在讓正確領域的博士生能夠回答，其他人則不行。最常用的子集是 `diamond` 子集，但自 2023 年發布以來，它也開始受到污染。

最後但同樣重要的是，名字浮誇但品質非常高的 [**Humanity's Last Exam**](https://agi.safe.ai/)（2024）包含 2.5K 個由各領域專家眾包的問題，涵蓋多個領域。它主要是私有的，問題需要複雜的知識和推理。它還沒有被破解，在我看來是一個很酷的資料集。唯一的問題是，由於沒有辦法快速為模型評分，人們現在透過使用大型語言模型評審來評估答案，而不是對照標準答案檢查，所以這是那些你在實際應用中會得到真正無法比較的結果的評估之一。

然而，儘管測試模型的潛在知識原始品質在幾年前很有意義（在訓練時測試模型品質仍然很有趣，預訓練時使用像 MMLU-Pro 這樣的評測，後訓練時使用 GPQA/HLE），我認為在接下來的幾年裡，我們會慢慢淘汰這類基準測試，有 2 個原因。

1. 它們對人類來說越來越難以理解：問題變得如此複雜，以至於對非專家來說幾乎不可能理解每個問題的表現意味著什麼（並確保資料集本身不包含錯誤）
2. 現在我們的模型已經連接到工具，例如網際網路存取，潛在知識評估越來越變成網路搜尋和檢索評估，所以它們本身的意義就減少了。簡而言之，我們正在從閉卷考試轉向開卷考試。作為比較，在法國的教育系統中，你在高中參加閉卷考試，但當你進入大學時，通常假設你可以存取資料庫、網際網路，評分不再是關於你背了什麼，而是更多關於在自由存取資訊的情況下你如何推理。我相信隨著模型能力的提升，這也是我們將在大型語言模型評估中看到的變化。

### 數學
數學評估資料集一直被用作推理和邏輯基準測試的代理，當然也獨立於檢查模型是否能解決數學問題。

兩個參考數學評估資料集是 [GSM8K](https://arxiv.org/abs/2110.14168)（2021），包含小學數學問題，以及 [MATH](https://arxiv.org/abs/2103.03874)（2021），網路上奧林匹亞問題的彙總，在過去幾年達到了飽和/污染狀態。前者被 [GSM1K](https://arxiv.org/abs/2405.00332)（2024）擴展，這是一個包含 1K 新問題的重建版本，用於測試哪些模型在前者上被污染，[GSM-Plus](https://arxiv.org/pdf/2402.19255)，一個使用對抗性變化（干擾項、數值變化等等）的模型改寫，以及 [GSM-Symbolic](https://arxiv.org/abs/2410.05229)（2024），較少使用，但非常有趣的 GSM8K 重寫為問題模板版本，以防止污染：問題可以無限再生。

社群現在一直專注於使用：
- MATH 的後續版本，[**MATH-500**](https://huggingface.co/datasets/HuggingFaceH4/MATH-500)（採樣的 500 個問題的代表性子集，以避免過度擬合）和 MATH-Hard（只有 500 個最難的問題）
- **AIME**（[24](https://huggingface.co/datasets/HuggingFaceH4/aime_2024)、[25](https://huggingface.co/datasets/math-ai/aime25)），美國高中生奧林匹亞資料集，在發布時按原樣採用。這些資料集很有趣，因為它們每年都由難度相當的新問題組成，透過比較發布時的結果與前一年資料集的結果來測試污染
- [**Math-Arena**](https://matharena.ai/)，定期更新的最新競賽和奧林匹亞彙編（它包含 AIME25，但也包含許多其他競賽！）

這些資料集中的大多數實際上不再「那麼難」，因為它們停留在小學程度（儘管 GSM-Symbolic 允許生成具有更多遞迴層級的問題，使它們在合成上更難）。在另一端，[FrontierMath](https://arxiv.org/abs/2411.04872)（2024）是試圖提供相當難的數學問題，由數學家為此場合單獨撰寫的。該資料集理論上是私有的（但似乎 OpenAI 已經存取了資料集的部分內容——真是遺憾）。[Humanity's Last Exam](https://agi.safe.ai/)（2025）（在知識部分介紹）也包含有趣的「為此場合製作」的數學問題，需要複雜的推理（特別是一些定理證明）。

我個人會在預訓練評估中使用 AIME25 和 MATH-500，在後訓練中使用 Math-Arena。

### 程式碼
由於智慧代理需要與工具互動，它們需要編程能力，要麼直接呼叫工具（如果它們是程式碼代理），要麼在出現問題時了解如何除錯工具輸出（對於程式碼和 json 代理都是如此，請參閱[此處](https://huggingface.co/learn/agents-course/en/unit2/smolagents/tool_calling_agents)的區別）。編程評估集也是推理的良好代理。

歷史上在 2021 年，程式碼評估集是 [MBPP](https://arxiv.org/abs/2108.07732)，1K 眾包的純 Python 入門級編程問題，[APPS](https://arxiv.org/abs/2105.09938)，從編程面試和分享網站策劃的 10K 程式碼生成問題，以及 [HumanEval](https://arxiv.org/abs/2107.03374)，與 Codex 模型一起引入，與前者相反，它由「專門為發布製作」的問題組成，在當時非常棒！它還配備了一個沙盒，以避免在評估者的機器上執行有問題的程式碼。（這篇論文介紹的最後一件事是 `pass@k` 的估算器，在此之前是透過字面檢查評估是否在 n 次中成功超過 k 次來計算的）。


[EvalPlus](https://openreview.net/pdf?id=1qvx610Cu7)（2023）團隊製作了 HumanEval+ 和 MBPP+，前者的擴展，透過增加更多測試案例和修復原始資料集中的錯誤以及增加更多輸入。[EvoEval](https://arxiv.org/abs/2403.19114)（2024）也引入了 HumanEval 的變體，透過語義重寫問題並增加難度標籤。

對於最終模型，你可能想要更難或未受污染的問題。

[**LiveCodeBench**](https://arxiv.org/abs/2403.07974)（2024）遵循類似的「從 leetcode 網站抓取」方法，但非常有趣，因為它儲存問題日期，以比較模型在完成訓練前後創建的問題上的表現。這是一個出色的無污染基準測試，我期待更新！

[**AiderBench**](https://aider.chat/docs/leaderboards/)（自 2024 年底開始線上我認為？）也使用來自現有編程網站的資料（具體來說是 Exercism），但超越了問題解決，特別測試程式碼編輯和重構。

對於後訓練，你需要更全面的評估，一些基準測試超越了對獨立問題的評估，這些問題沒有評估複雜的編程能力。[RepoBench](https://arxiv.org/abs/2306.03091)（2023）測試 Python 或 Java 中的儲存庫級自動完成系統，使用來自 Github 的程式碼作為來源。它是透過遮罩程式碼庫中的隨機行並要求完成來建立的，無論是跨檔案還是檔案內函數，並定義了幾個測試級別（檢索、完成、組合）。

[**SweBench**](https://openreview.net/pdf?id=VTF8yNQM66)（2024）是一個更知名和完整的版本，也使用 github，但這次測試模型是否可以解決現有問題，因此邏輯理解、跨檔案編輯和執行、長上下文推理等等。

此時，我建議關注 LiveCodeBench、AiderBench 和 SWE-Bench 的更高品質子集（SWE-Bench verified），並閱讀 [METR 報告](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)關於實際程式碼助理有用性的內容。

### 長上下文
為了正確地與使用者進行長時間討論而不失去追蹤，你需要良好的長上下文管理。（有趣的是，3 年前，模型的最大上下文長度是 2048 個 token，而現在我們基本上都在 128K 及以上）。

2023 年開始測試這一點的評估可能是 [NIAH](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)（大海撈針），你在長篇無關文字中放置一個隨機事實，並要求模型檢索它。它提供了一個整潔的框架來評估模型在上下文中的哪個位置最有可能忘記東西，以及從哪個上下文長度開始。2023 年模型在這方面真的很糟糕，2025 年它接近被解決。

此後出現了更複雜的長上下文擴展。[RULER](https://arxiv.org/pdf/2404.06654)（2024）增加了多跳追蹤（要求模型跟隨變數鏈以獲得正確值）、詞頻變化，並增加了 NIAH 的 QA 變體。它現在也接近被解決。[Michelangelo](https://arxiv.org/pdf/2409.12640v2)（2024，有時也稱為 MRCR 用於多輪共同參照）也使用合成長上下文資料：任務（長度不同）測試模型是否可以精確再現上下文的唯一部分（以及識別相關資訊是否存在）並理解對文字的修改序列。然後在 [OpenAI MRCR](https://huggingface.co/datasets/openai/mrcr)（2025）中進行了擴展。[InfinityBench](https://arxiv.org/abs/2402.13718)（2024）是多語言的（英語和中文），並提供 100K token 合成資料任務，涵蓋各種目標（QA、檢索如 NIAH、非常長上下文的計算……）。InfinityBench 仍然提供一些訊號。

[**HELMET**](https://arxiv.org/abs/2410.02694)（2024）結合任務和現有基準測試，以獲得一個更大的單一資料集，具有更多訊號：RAG 和 QA 資料集（Natural questions、TriviaQA、PopQA、HotpotQA、Narrative QA 和 InfinityBench）、召回（RULER 和 JSONKV）、帶引用的生成（ALCE 的子集）、摘要、重新排序段落（MS MARCO）、上下文學習（TREC、NLU、Banking77、CLINIC150）。基準測試彙總是詳盡的，但存在測量兩次相同內容的風險：例如，不要同時針對 HELMET 和 InfinityBench 測試你的模型，然後彙總結果，因為你會執行兩次相同的評估！在 2025 年，它仍然有足夠的區分能力來比較模型。

我最喜歡的長上下文評估想法是 [Novel Challenge](https://arxiv.org/abs/2406.16264)（2024），關於去年出版的虛構書籍的 1K 真/假陳述（由這些書的讀者提供！）需要閱讀並理解全文才能正確回答，以及 [**Kalamang 翻譯資料集**](https://arxiv.org/abs/2309.16575)（2024），模型需要透過閱讀語法書從英語正確翻譯成 Kalamang（Kalamang 是一種資源如此稀少的語言，以至於它沒有線上存在——只有 200 個使用者）。Kalamang 翻譯集特別可以擴展到其他低資源語言（但如果能擴展使用基於規則的語法檢查器來測試生成有效性以獲得嚴格的準確性而不是依賴 BLEU，那會很酷……）。

### 指令遵循
兩個主要的指令遵循資料集是 [**IFEval**](https://arxiv.org/abs/2311.07911)（2023）及其擴展 [**IFBench**](https://arxiv.org/abs/2507.02833)（2025）。IFEval 是近年來最聰明的評估想法之一，在我看來：模型被要求遵循格式指令（關於關鍵字、標點符號、字數/句數、檔案類型格式如 markdown 或 html 等等）。這些條件中的每一個都可以用特定的解析測試來檢查：這意味著這個評估是少數幾個可以在不依賴模型評審的情況下獲得嚴格分數的自由形式生成評估之一。

更一般地說，它屬於功能正確性/單元測試評估類型，這是我個人最喜歡的評估模型方式。它也非常容易再生或擴展以防止污染。

順帶一提，但一些基準測試也測試「非指令遵循」（不合規）：[CoCoNot](https://www.arxiv.org/pdf/2407.12043)（2024）特別測試模型是否會或不會遵守不完整（未充分指定/不清楚）、無法回答（由於缺乏資訊或 AI 人性化，通常觸發幻覺）或不安全的請求。它使用手動查詢撰寫、模型撰寫不合規請求，然後過濾以創建一個呈現為分類問題的評估集。

### 工具呼叫
工具的出現是開始將大型語言模型推向智慧代理領域的特性之一。

[**TauBench**](https://arxiv.org/pdf/2406.12045)（2024）評估模型在零售和航空領域回答使用者查詢的能力（訂購/預訂/尋找產品等等）。資料庫使用合成樣本模擬真實領域資料，當 1) 其操作正確更新資料庫 2) 它適當地回答使用者時，模型被認為是正確的。為了使這個基準測試自動化，使用者由大型語言模型模擬，這使得這個評估運行成本相當高且容易出錯。儘管有這些限制，它還是相當常用，特別是因為它很好地反映了真實使用案例。

[ToolBench](https://arxiv.org/pdf/2305.16504)（2023）需要呼叫 API（OpenWeather、Cat、HomeSearch、TripBooking、GoogleSheets、WebShop、Tabletop 等等）來解決資料集中的 100 個測試案例，需要 1 到 10 次工具呼叫來解決。這些 API 中的一些是模擬的，一些是真實的，這使得資料集容易意外失敗。因此在 [StableToolBench](https://arxiv.org/pdf/2403.07714)（2025）中進行了修復和擴展，它引入了一個通用的 VirtualAPIServer 模擬一切以確保評估穩定性，但依賴大型語言模型評審進行評估，引入了另一層偏差。

[**BFCL**](https://openreview.net/pdf?id=2GmDdhBdDk)（2025，但基準測試實際上已經有幾年了）在這一年中有了相當大的發展，在其當前版本中包含 4 個子集：單輪（簡單工具呼叫）、來自使用者的眾包真實生活函數呼叫、多輪對話（測試長上下文中的準確性和使用工具呼叫的查詢回答）以及智慧代理（網路搜尋、記憶體、sql 資料互動）。它使用抽象語法樹、執行回應和狀態匹配（最終狀態是否為預期狀態）的組合來評估呼叫是否正確。人們專注於 v3 來特別測試工具呼叫，v4 測試網路和搜尋工具使用。

最後，隨著 MCP 的創建，一些基準測試出現來測試 MCP 導向的工具呼叫——然而大多數主要依賴模型評審，並使用真實世界的 API，這可能由於網路問題引入潛在的失敗案例/缺乏可重複性（看起來網站創建者的額外負載不是太大的問題，因為涵蓋的大多數 MCP 的使用者基礎足夠大）。

[MCPBench](https://arxiv.org/abs/2508.20453)（2025）將大型語言模型連接到即時、真實世界的 MCP 伺服器（Wikipedia、HF、Reddit、Steam、arxiv……）與需要多輪解決的任務（合成創建）。評估結合了基於規則的工具呼叫有效性和成功檢查，以及大型語言模型評審來評估查詢是否得到正確回答。

[**MCP-Universe**](https://arxiv.org/abs/2508.14704)（2025）使用 11 個 MCP 伺服器，涵蓋各種真實世界主題（IRL 導航、3D 設計、網路搜尋等等）。這個資料集很酷的地方在於評估依賴於幾個嚴格的評估器，一個用於格式正確性，兩個用於答案正確性：由於任務可以是靜態的（詢問不變的事物）或動態的（儲存庫中的 github 星星、天氣……），在後一種情況下，答案正確性使用任務依賴的基於執行的評估框架，該框架自動從相關來源抓取最新的正確答案並將模型輸出與之比較。這比依賴大型語言模型評審要整潔得多！

[**LiveMCPBench**](https://arxiv.org/abs/2508.01780)（2025）提供大量可本地部署的 MCP 伺服器集合，以測試模型在完成任務時區分工具的能力。最佳模型已經達到 80%——所以我們接近飽和。然而，測試模型是否可以在非常長的列表中選擇適當的工具是一個良好的使用案例，隨著網路走向 mcp，這將變得越來越重要。

（順便說一句，這裡有一個關於如何為智慧代理編寫良好工具的很酷的[文件](https://www.anthropic.com/engineering/writing-tools-for-agents)。）

雖然測試個別能力提供了有價值的訊號，但真實世界的助理表現來自這些能力如何結合。一個模型可能擅長推理，但當推理必須與工具呼叫和長上下文管理同時整合時可能會失敗，所以我們需要需要同時協調多種能力的評估。

## 助理任務
我相信**助理任務**將成為下一級評估的主要方式之一：解決它們需要多種能力的組合（長上下文、推理、工具呼叫……），而基準測試本身在有用的真實世界設定中提供了特定領域表現的洞察。它們也往往比特定能力基準測試更容易理解（對一般大眾）。如果基準測試足夠通用，它們不會檢查使用了哪些精確工具，而是檢查最終結果是否正確，因為複雜任務允許多條成功路徑。

### 真實生活資訊檢索
[**GAIA**](https://arxiv.org/abs/2311.12983)（2023）透過要求模型使用工具、推理和檢索的組合來解決真實生活查詢（有時包括文件），開啟了現代智慧代理評估。問題被分為 3 個級別，第一個現在已經飽和，第三個對模型來說仍然很難。它也是那些你會發現數字根據評估方法分散的基準測試之一，因為人們要麼報告公開驗證集，要麼使用大型語言模型評審針對私有測試集進行評估（當有公開排行榜[在這裡](https://huggingface.co/spaces/gaia-benchmark/leaderboard)時）。

它後來在 [BrowseComp](https://cdn.openai.com/pdf/5e10f4ab-d6f7-442e-9508-59515c65e35d/browsecomp.pdf)（2025）中被複製，它測試相同的東西（模型是否可以使用工具和線上資訊找到特定查詢的適當答案），但不保證結果的唯一性，因為問題是從結果開始構建的，並從中建立問題，具有不同的難度級別：例如，從要檢索的特定論文，將透過組合元資料資訊創建問題，例如「哪篇關於主題的論文在會議上發表，有一個國籍作者和兩個來自實體的人？」然而，該基準測試目前可能也更難。

最後，[GAIA2](https://huggingface.co/blog/gaia2) 超越了簡單的資訊檢索，使用模擬行動環境來測試助理如何正確回答依賴事件鏈和工具呼叫的查詢。目前，時間敏感和故意嘈雜的子集（模擬失敗的 API 呼叫）對模型來說最難，而搜尋和執行對 SOTA 模型來說似乎非常容易。

### 科學助理
[SciCode](https://arxiv.org/abs/2407.13168)（2024）測試模型是否可以透過撰寫適當的科學程式碼來解決真實生活科學問題，涵蓋 stem 領域（從生物學到數學/化學/……）。問題來自真實生活工作流程，每個核心問題都分解為更簡單的子問題。對於第一個版本，評估由科學家和模型評審完成——模型在發布時相當糟糕（得分低於 5%），但我不確定在哪裡可以找到最新的結果。

[PaperBench](https://arxiv.org/abs/2504.01848)（2025）同樣測試模型是否可以複製 ML 研究，但這次使用更難的設定：給定 ICML 高品質論文，模型必須重建匹配的程式碼庫（8K 個單獨評分的任務由這些論文的作者貢獻，分組為帶有最終成績權重的評分樹）。基準測試使用大型語言模型評審進行評估（儘管我懷疑其中一些可以透過稍微約束要求的程式碼形狀來自動完成）。

[DSBench](https://arxiv.org/pdf/2409.07703)（2025）是一個使用 Kaggle 和 ModelOff（金融資料）樣本的多模態資料分析基準測試。從附錄中的範例來看，似乎來自 ModelOff 的問題在多選設定中提供，這可能使任務更容易，而 Kaggle 任務各有自己的指標。

[**DABStep**](https://arxiv.org/abs/2506.23719)（2025）使用真實生活問題和資料評估模型在以前私有（因此未受污染）的操作資料分析工作負載上的表現。所有問題都需要多步驟推理和各種文件解析，當然還有特定的資料操作技能。這是一個整潔的評估，因為它很難並複製了實際有用的真實世界使用案例，而且因為每個問題都有標準答案，所以評估是無偏的且成本不高。

助理任務測試實際場景中的整合能力，但它們要麼是動態且唯讀的，要麼是在不變的環境中是靜態的。為了評估適應性和動態決策，我們需要能夠「驚訝」模型的環境。

## 基於遊戲的評估
**基於遊戲**的基準測試非常有趣，有幾個原因：它們通常評估對變化環境的適應性（與大多數靜態的助理任務相反）、需要長上下文推理，最後但同樣重要的是，對大多數人來說是**可理解的**。然而，它們既不紮根於真實生活，也不一定反映實際有用使用案例的良好表現。

這些中最著名的正式評估可能是 [ARC-AGI](https://arcprize.org/)。第一個版本（2019）由序列中的拼圖網格組成，模型必須在沒有提供明確規則的情況下找到該序列的最後一項。這個基準測試在我看來非常讓人想起邏輯導向的智商測試，它在 2024 年幾乎被解決了。類似的基準測試（規則外推）是 [Baba is AI](https://arxiv.org/abs/2407.13729)（2024）。最新版本的基準測試，ARC-AGI3（2025，正在進行中），仍在開發中，包含專門為基準測試製作的全新遊戲（需要探索、複雜規劃、記憶體管理……）。它仍在進行中，目前在可用問題上的最佳解決方案是暴力破解遊戲。

社群和模型提供者使用大型語言模型探索了許多現有遊戲。單人冒險遊戲/RPG，如 [TextQuests](https://huggingface.co/blog/textquests)（2025）或 [Pokemon](https://github.com/benchflow-ai/benchflow/tree/main/libs/pokemon-gym)（2024）（例如 [Claude](https://www.twitch.tv/claudeplayspokemon) 和 [Gemini](https://www.twitch.tv/gemini_plays_pokemon) 的 Twitch）需要非常長遠的規劃組合來獲得目標，這需要足夠的長上下文記憶體管理、推理和回溯能力。單人生存遊戲如 [Crafter](https://arxiv.org/abs/2109.06780)（2021，受 Minecraft 啟發）需要相同的能力。許多單人遊戲環境已被整合到 [Balrog](https://arxiv.org/pdf/2411.13543)（2024）基準測試中。

競爭性虛張聲勢遊戲，如 [Poker](https://arxiv.org/html/2501.08328v1)（2025）或 Mafia 變體，如 [Town of Salem](https://github.com/summersonnn/Town-Of-Salem-with-LLMs)（2025）和 Werewolf（2025，[這裡](https://arxiv.org/abs/2407.13943)/[那裡](https://werewolf.foaster.ai/)），非常有趣地測試邏輯、推理以及欺騙能力。例如，Claude Opus 4 無法以吸血鬼（欺騙性角色）身分贏得 Town of Salem，但作為農民（非欺騙性角色）表現良好。合作遊戲如 Hanabi 也可以用來測試受限環境中的適應性和溝通能力。

這些的另一個非常整潔的地方是它們有一個單一且明確的通過/失敗指標：大型語言模型是否贏得了遊戲？目前，如果我要使用這些來評估模型，我可能會查看 TextQuests 的能力和 Town of Salem 的安全性。

除了在受控環境中測試能力之外，還有一種評估類型本質上是不可能作弊的：預測未來。（好吧，這是一個離題，但我覺得這些超級有趣，它們可能是相關的！）

## 預測者
在過去的一年裡，出現了一個新的不可能污染的任務類別：預測。（我猜從技術上講，股市預測可以透過一些操縱來作弊，但希望我們還沒有達到在評估上搞亂的財務激勵）。它們應該需要跨來源的推理組合來嘗試解決關於尚未發生的事件的問題，但不確定這些基準測試是否具有足夠的區分度以具有強大的價值，它們可能會加強大型語言模型的「老虎機成功」氛圍。（對某些事件的表現接近隨機是因為它們無法預測還是因為模型很糟糕？在另一個方向，如果模型能夠正確預測事件，問題是太容易還是太公式化？）

[FutureBench](https://huggingface.co/blog/futurebench) 測試模型是否可以預測未來值得報導的事件。它使用 2 個來源：瀏覽和大型語言模型生成具有每週時間範圍的問題，以及來自博彩市場的使用者預測。所有資料在使用前都經過大量過濾和清理。目前，模型在人類創建的賭注上勉強優於隨機，並在模型生成的問題上成功 3/4 次（可能更容易）。

[FutureX](https://arxiv.org/abs/2508.11987) 類似，但使用一系列特定網站（預測市場、政府網站、一般排名網站和即時資料平台），然後使用模板生成關於潛在未來事件的問題（`股票何時達到點？`）。每天生成 500 個問題，過濾意外無關的問題。

[Arbitrage](https://arxiv.org/pdf/2412.18544) 使用類似的方法生成問題，核心區別在於時間範圍：那裡的事件應該在 2028 年解決。

## 透過評估獲得真正有用的模型

評估的格局隨著能力的飛躍而演變，從測試孤立技能到衡量更實際場景中的整合表現。

截至 2025 年 9 月，我建議使用：

- **核心能力**（針對模型建構者）：訓練用的舊能力評測，後訓練用 MATH500/AIME24、GPQA、IFEval、SWE-Bench、你選擇的長範圍評估如 HELMET、如果你針對工具使用則用 TauBench 或 BFCL
- **核心能力**（用於在推理時比較模型）：IFBench、HLE、MathArena、AiderBench 和 LiveCodeBench、MCP-Universe
- **長期任務**（用於真實世界表現）：GAIA、DABStep、SciCode，或你的使用案例的特定領域評估
- **遊戲**（用於衡量穩健性和適應性的一些額外樂趣）：ARC-AGI3 出來時、TextQuests、如果你對安全性感興趣則用 Town of Salem，或任何你喜歡的超越 Poker/Chess/Go 的其他遊戲。

該領域正在朝著測試能力協調而不是孤立技能的評估發展，以實際使用。這符合我們建構「運作良好」的模型的目標——能夠可靠地結合核心能力、工具使用和良好協調以解決實際問題的系統。

順便說一句：我希望該領域更加強調功能測試而不是模型評審，以及通常可理解的資料集和任務。
