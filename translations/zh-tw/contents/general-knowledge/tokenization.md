# Tokenization（分詞）

## 為什麼要對文字進行分詞？如何進行分詞？
由於大型語言模型本質上是巨大的數學函數，它們處理的是數字，而不是文字。

假設您想要將一個句子轉換成數字。首先需要決定如何將句子切割成小片段，然後將每個小片段對應到一個數字；這就是 *tokenization（分詞）*。

在過去，人們會嘗試將文字中的每個字元與其在字母表中的索引對應（`a` -> 1、`b` -> 2 等），這稱為 *字元級分詞*（在字元之間進行切割）。而在另一個極端，人們也嘗試將每個單詞與其在字典中的索引對應（`a` -> 1、`aardvark` -> 2、`ab` -> 3 等），這稱為 *詞級分詞*（在空格處切割，如果您的語言有空格的話 - 如果沒有，就會比較困難）。

這兩種方法都有一個嚴重的限制：它們會從輸入文字中移除資訊。它們會抹除從詞形中可以看出的語義連結（例如：`dis similar`、`similar`、`similar ity`、`similar ly`），而這些是我們希望模型保留的資訊，這樣它才能將相關的詞彙連結在一起。
（此外，如果輸入中突然出現一個全新的詞彙會怎樣？它無法獲得數字，您的模型就無法處理它 😔）

因此，有些人想到將詞彙切割成子詞，並為這些子詞分配索引（`dis`、`similar`、`ity`、`ly`）！

這最初是使用形態句法規則來完成的（「形態句法」類似於詞彙構成的語法）。現在大多數人使用位元組對編碼（Byte Pair Encoding, BPE），這是一種聰明的統計方法，可以根據子詞在參考文字中的出現頻率自動建立子詞。

總結一下：分詞是一種將文字的小單位（可以是一個或多個字元，最多到詞彙層級）對應到數字（類似於索引）的方法。當您想要處理文字時，您的輸入文字（在推論時稱為 *prompt（提示詞）*）會由分詞器切割成這些 *tokens（標記）*。模型或分詞器能夠解析的所有標記範圍稱為其 *vocabulary（詞彙表）*。
#### 深入了解：理解分詞
我建議深入閱讀前兩個連結之一。
- ⭐ [🤗 NLP 課程中關於不同分詞方法的說明](https://huggingface.co/learn/nlp-course/en/chapter2/4)
- ⭐ [🤗 文件中關於分詞的概念指南](https://huggingface.co/docs/transformers/en/tokenizer_summary)
- [Jurafsky 關於分詞（和其他主題）的課程](https://web.stanford.edu/~jurafsky/slp3/2.pdf) - 方法較為學術性，可跳到 2.5 和 2.6 節（其餘部分也很有趣，但範圍太廣）

#### 深入了解：位元組對編碼
- ⭐ [🤗 NLP 課程中關於 BPE 的說明](https://huggingface.co/learn/nlp-course/en/chapter6/5)
- [將 BPE 引入 NLP 的論文](https://aclanthology.org/P16-1162/)


## 分詞的諸多問題
### 選擇正確的詞彙表大小
詞彙表的大小表示模型需要學習多少個別的標記（例如子詞）。

**過大**的詞彙表可能會將一些非常罕見的詞作為完整標記包含進來（例如：`aardvark`），這可能導致兩個問題。

如果這樣的罕見詞幾乎從未在訓練資料中出現，它可能難以與其他概念連結，模型可能無法推斷它的含義。

另一方面，如果它很少出現且僅出現在特定上下文中，它可能會與一些非常特定的其他詞連結：例如，如果您使用論壇資料進行訓練，而您的分詞器將某個使用者名稱作為單一標記對應到詞彙表中，您的模型可能會將這個標記與該特定使用者的內容關聯起來。

**過小**的詞彙表則會帶來另外兩個問題：更差的表示能力，以及推論時成本增加。

讓我們回到上面的例子，我們對從 `similar` 衍生的詞進行分詞。使用偽 BPE 方法（大詞彙表）對 `similarly` 進行分詞，會將該詞切割成 2 個標記（`similar`、`ly`）。如果我們改用字元級分詞（因此詞彙表非常小，大小與字母表相當），同樣的詞會被切割成 9 個標記（`s`、`i`、`m`、`i`、`l`、`a`、`r`、`l`、`y`）。

第一種方法將 `similarly` 切割成具有個別語義意義的標記，而第二種方法則不然：詞彙表太小時，我們失去了一些語義表示。表示長度的差異也意味著使用較小的詞彙表生成我們的詞的成本要高出許多倍（需要 9 個標記而不是 2 個，因此成本高出 5 倍！）。

目前，大多數人似乎使用詞彙表大小的啟發式方法，這似乎與涵蓋的語言數量和模型大小相關，因此使用接近類似大小的參考模型的標記數量可能適合您。
#### 深入了解：罕見標記效應
- [Less Wrong 上的 SolidGoldMagikarp 文章](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)
	- 關於一些人如何在 OpenAI 的詞彙表中識別出非常罕見的標記的非常有趣的閱讀 - 這相當酷，因為這是在沒有存取模型內部的情況下完成的（例如我們不知道訓練資料包含什麼）
- [Fishing for Magikarp，Cohere 的論文](https://arxiv.org/abs/2405.05417)
	- 關於檢測這些標記的後續工作

### 處理多種語言
（建議：在閱讀本節之前先閱讀 BPE 的說明）
在建構或選擇分詞器時，您會從參考文字中建構詞彙表。這意味著您的分詞器會知道來自此參考文字的詞彙和字元。通常，這意味著使用英語資料和拉丁字母。

如果您想要新增新語言，而您的新語言使用相同的文字系統並共享一些詞根，理論上您可以希望原始語言的一些語義能夠轉移到新語言。

然而，如果您想要讓分詞器正確切割其他語言的文字（特別是用其他文字系統書寫的語言），您最好在建構該分詞器時包含來自這些語言的資料。但大多數時候，這些資料會包含不平衡比例的初始語言（例如：英語）與新語言（例如：泰語或緬甸語），初始語言的比例要高得多。由於現今使用的最有效率的分詞器方法（如 BPE）會根據最常見的詞彙建立其複雜的詞彙表標記，大多數長標記將是英語詞彙 - 而來自較不常見語言的大多數詞彙只會在字元層級進行切割。

這種效應導致多語言分詞的不公平性：某些（較不常見或*資源較少*的）語言需要多個數量級的標記才能生成與英語等長的句子。

#### 深入了解：語言與分詞
- ⭐ [Yennie Jun 關於跨語言分詞問題的精彩分析與示範](https://www.artfish.ai/p/all-languages-are-not-created-tokenized)
	- 分析本身非常清楚，值得在 [示範空間](https://huggingface.co/spaces/yenniejun/tokenizers-languages) 中試玩
- ⭐ [Aleksandar Petrov 關於分詞不公平性的示範](https://aleksandarpetrov.github.io/tokenization-fairness/)
	- 我建議查看 `Compare tokenization of sentences` 以了解不同語言的推論成本差異

### 那數字呢？
在建構分詞器時，您需要決定如何處理數字。您是只索引 0 到 9，並假設所有其他數字都是數字的組合，還是您想要單獨儲存數字，例如最多到十億？目前知名的模型在這方面展現了一系列不同的方法，但尚不清楚什麼方法更有利於數學推理。也許需要新的分詞方法，例如階層式分詞，來解決這個問題。
#### 深入了解：數字分詞
- ⭐ [Yennie Jun 的視覺化示範，展示 Anthropic、Meta、OpenAI 和 Mistral 模型的分詞器如何切割數字](https://www.artfish.ai/p/how-would-you-tokenize-or-break-down)
- [Beren Millidge 關於數字分詞多年演變的簡短歷史](https://www.beren.io/2024-05-11-Integer-tokenization-is-now-much-less-insane/)
