# 技巧與訣竅
以下是使用人工標註者建立評估資料集時，您可能會想考慮的一些實用建議。如果您還沒讀過「使用人工標註者」這一頁，我們建議您先閱讀該頁面，然後再回來閱讀本頁。

## 設計標註任務

- **簡單為上**：標註任務可能會變得過於複雜，因此請盡可能保持簡單。將標註者的認知負荷降到最低，有助於確保他們保持專注，並產出更高品質的標註結果。

- **檢查顯示的內容**：只顯示標註者完成任務所需的必要資訊，並確保不包含任何可能引入額外偏誤的內容。

- **考量標註者的時間**：內容的顯示位置和方式可能會增加額外的工作量或認知負荷，進而對結果品質產生負面影響。例如，確保文字和任務能同時顯示在畫面上，避免不必要的捲動。如果您結合多個任務，且其中一個任務的結果會影響另一個，可以依序顯示它們。仔細思考標註工具中所有內容的顯示方式，看看是否還能進一步簡化。

- **測試設置**：當您設計好任務並制定好初步指引後，在讓整個團隊參與之前，請務必先自己在幾個樣本上測試，並視需要進行迭代調整。

## 標註過程中

- **標註者應獨立作業**：標註者在執行任務時最好不要互相協助或看到彼此的工作成果，因為這樣會傳播個人偏誤並導致標註漂移。團隊的一致性應該始終透過完善的指引來達成。您可能需要先讓新成員在獨立的資料集上進行訓練，並/或使用標註者間一致性指標來確保團隊的方向一致。

- **一致性是關鍵**：如果您對指引做了重大變更（例如修改了定義或指示，或新增/移除了標籤），請考慮是否需要重新審視已標註的資料。至少，您應該透過詮釋資料（例如 `guidelines-v1`）來追蹤資料集中的變更。

## 混合人機標註

有時團隊在時間和資源上面臨限制，但又不想犧牲人工評估的優點。在這些情況下，您可以利用模型的協助來提高任務效率。

- **模型輔助標註**：您可以使用模型的預測或生成結果作為預標註，讓標註團隊不需要從頭開始。但請注意，這可能會將模型的偏誤引入人工標註中，而且如果模型的準確度不佳，反而可能增加標註者的工作量。

- **監督模型評判**：您可以結合模型評判方法的威力（請參閱「模型評判」章節）和人工監督者來驗證或捨棄結果。請注意，「人工評估的優缺點」中討論的偏誤在此仍然適用。

- **識別邊緣案例**：為了讓任務進行得更快，可以使用模型陪審團，然後讓人工監督者在模型意見不一致或需要打破僵局時介入。同樣地，請留意「人工評估的優缺點」中討論的偏誤。

## 端對端教學

若要根據這些建議建立您自己的自訂評估設置，您可以參考 Argilla 提供的這份[實務教學](https://github.com/argilla-io/argilla-cookbook/tree/main/domain-eval)。它會引導您使用合成資料以及 [Argilla](https://github.com/argilla-io/argilla/) 和 [distilabel](https://github.com/argilla-io/distilabel) 進行人工評估，為您的領域建立自訂評估任務。本指南從領域文件開始，最後產生一個自訂評估任務，您可以使用 [lighteval](https://github.com/huggingface/lighteval) 來評估您的模型。
