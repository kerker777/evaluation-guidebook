# 基礎概念

## 什麼是人工評估？
人工評估就是請人類來評估模型的表現。
在本文中，我們將探討事後評估：當模型已經訓練完成、你有特定的任務需求時，由人類來提供評分。

### 系統化評估
系統化的人工評估主要有 3 種方式。

如果**你還沒有資料集**，但想探索一組特定能力，你可以提供人類一個任務和評分準則（例如：`試著讓這兩個模型輸出有害內容；如果模型輸出有害內容則得 0 分，否則得 1 分`），並提供一個（或多個）可供互動的模型，然後請他們提供評分和理由。

如果**你已經有資料集**（例如：`一組你希望確保模型不會回答的提示詞`），你可以用這些提示詞來詢問你的模型，然後將提示詞、輸出結果和評分準則提供給人類評估員（`如果模型洩漏私人資訊則得 0 分，否則得 1 分`）。

最後，如果**你已經有資料集和評分結果**，你可以請人類透過[錯誤標註](https://ehudreiter.com/2022/06/01/error-annotations-to-evaluate/)的方式來檢視你的評估方法（*這也可以當作上述類別的評分系統*）。這是測試新評估系統時非常重要的步驟，但嚴格來說這屬於「評估一個評估系統」的範疇，因此稍微超出本文的討論範圍。

備註：
- *對於已經部署的正式環境模型，你也可以向使用者徵求回饋，並進行 A/B 測試。*
- *[AI 稽核](https://arxiv.org/abs/2401.14462)（外部系統化的模型評估）通常是基於人工的，但不在本文的討論範圍內。

### 非正式評估
還有兩種以較非正式方式進行人工評估的方法。

**感覺測試（Vibes-checks）**是由個人進行的手動評估，通常使用未公開的提示詞，以整體感受模型在多種使用情境下的表現（從程式設計到撰寫文章的品質）。這類測試結果常在 Twitter 和 Reddit 上分享，大多屬於個人經驗分享，容易受到確認偏誤的影響（換句話說，人們往往會找到他們想找的結果）。不過，這些測試可以是[你自己使用情境的良好起點](https://olshansky.substack.com/p/vibe-checks-are-all-you-need)。

**競技場（Arenas）**是群眾外包的人工評估方式，用來為模型排名。
一個知名的例子是 [LMSYS 聊天機器人競技場](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)，社群使用者會與模型聊天，直到他們認為其中一個模型比另一個更好為止。然後這些投票會被彙整成 Elo 排名（一種比賽排名系統），以選出「最佳」模型。

## 人工評估的優缺點

人工評估有以下幾個優點：
- **彈性**：只要你定義得夠清楚，幾乎任何東西都可以評分！
- **不會有資料污染問題**：如果你請人類撰寫新的問題來測試你的系統，這些問題應該不會出現在你的訓練資料中（希望如此）
- **與人類偏好的相關性**：這點相當明顯，因為評分本來就是由人類進行的。
  *備註：不過，在進行人工評估時，你需要確保評估員的多元性，讓你的結果能夠具有普遍性。*

然而，人工評估也有一些限制：
- **第一印象偏誤**：人類評估員傾向於[根據第一印象](https://arxiv.org/abs/2309.16349)來評估答案的品質，而非實際的事實性或可信度。
- **語氣偏誤**：群眾外包的評估員特別容易受到語氣影響，會低估語氣堅定的答案中事實或邏輯錯誤的數量。換句話說，如果模型以自信的語氣說出錯誤的內容，人類評估員比較不容易察覺，這可能導致評分偏向較自信的模型。（專家評估員比較不容易受到這些偏誤影響。）
- **自我偏好偏誤**：人類[最有可能偏好迎合他們觀點或符合他們意見或錯誤的答案](https://arxiv.org/abs/2310.13548)，而非事實正確的答案。
- **身分認同偏誤**：不同身分背景的人往往有不同的價值觀，對模型答案的評分會有很大差異（例如在[有害內容](https://arxiv.org/abs/2205.00501)的評估上）

### 系統化人工評估
系統化人工評估的優點，特別是使用付費評估員時：
- **取得高品質的資料**，這些資料是針對你的使用情境而設計的，你之後可以在此基礎上繼續發展（例如如果你需要開發偏好模型）
- **資料隱私**：如果你仰賴付費的人工評估員，特別是內部評估員，你的資料集應該相對安全，而使用閉源 API 模型的 LLM 評估則較無法保證你的資料會如何被處理，因為你是將資料傳送到外部服務。
- **可解釋性**：模型獲得的分數可以由評估員來解釋。

系統化人工評估也有一些額外的問題：
- **成本**：如果你正確支付評估員薪資，成本可能很快就會變得很高。而且你很可能需要多輪的迭代評估來改善你的準則，這會增加成本。
- **難以擴展**：除非你是在評估一個有使用者回饋的正式環境系統，否則人工評估實際上很難擴展，因為每一輪新的評估都需要動員新的評估員（並支付他們薪資）。
- **缺乏可重現性**：除非你持續使用完全相同的評估員，且你的準則完全明確無誤，否則某些評估結果可能很難精確重現。

### 非正式人工評估
非正式人工評估的優點：
- **較低的成本**：因為你仰賴群眾的善意
- **發現邊界案例**：由於你以幾乎無限制的方式善用使用者的創意，你可以發現有趣的邊界案例
- **更好的可擴展性**：只要你有許多有興趣且願意參與的人，非正式人工評估的擴展性較佳且進入門檻較低

非正式方法（沒有評估員篩選機制）的明顯問題：
- **高度主觀性**：很難讓許多社群成員使用廣泛的準則來維持一致的評分標準，特別是評估員的偏好往往[受文化影響](https://arxiv.org/abs/2404.16019v1)。我們可以期待透過大量投票的「群眾智慧」效應（參見 Galton 的維基百科頁面），這些影響能被平滑化。
- **不具代表性的偏好排名**：由於年輕西方男性在網路技術社群中過度代表，可能導致偏好非常偏頗，與一般大眾的偏好不符，無論是在探討的主題還是整體排名上都是如此。
- **容易被操控**：如果你使用未經篩選的群眾外包評估員，第三方很容易操控你的評估，例如提高特定模型的分數（因為許多模型都有獨特的寫作風格）
