# 重現性問題排解

假設您閱讀了一篇關於某個很酷的新模型的最新技術報告，而您想在自己的機器上重現其結果...但是卻做不到？
讓我們來探討原因。

## 不同的程式碼基底
要精確重現評估分數到小數點，您首先需要確保使用的程式碼基底與您想要重現的論文完全相同。

通常，這意味著要使用作者提供的預設評估程式碼，或是參考函式庫中的標準實作，例如 Eleuther 的 AI `lm_eval` 或 HuggingFace 的 `lighteval`。然而，如果沒有提供評估的程式碼來源，那麼很抱歉，您不太可能精確地重現結果。

如果您想輕鬆了解使用不同實作時會出現什麼樣的差異，可以探索我們與 HuggingFace 評估團隊撰寫的[這篇部落格](https://huggingface.co/blog/open-llm-leaderboard-mmlu)（⭐）。它研究了我們在 MMLU 評估的 3 種常見實作（`lm_eval`、`helm` 和原始作者的實作）之間觀察到的差異，以及這些差異如何改變模型分數。

*注意：正是基於這個原因，Hugging Face 團隊決定推出 [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)，以獲得統一且同質的模型分數比較，從而與內部實驗進行比較。*

### 實作可能不同的其他細微之處
我們觀察到以下幾點即使使用相同的程式碼基底也很容易出錯：
- **不同的隨機種子。**
	- 通常，推論比訓練更少受到隨機種子的影響。然而，它們仍然可以影響某些 CUDA 操作（請參閱 PyTorch 關於[可重現性](https://pytorch.org/docs/stable/notes/randomness.html)的頁面），並在您使用非貪婪生成策略時改變預測結果。它們也可能影響 few-shot 時使用的提示，以及某些前處理或後處理函數。
	  -> 微小的改變可能導致幾個百分點的差異。
- **實際上不同的指標**。
  即使指標名稱相同，實際上它們可能不同。一些例子：
	- 如果原始實作是*對數似然*的 `exact match`（計算不同可能答案的對數機率），而您使用的是*生成式*的 `exact match`（僅將主要的貪婪生成結果與參考答案比較），您將不會得到相同的分數。
	- 我們還在評估程式碼基底中看到許多任務被定義為 `exact match`，但實際上是 `prefix exact match`（僅比較生成結果的開頭與參考答案）、`suffix exact match`（相反）或 `quasi exact match`（帶有正規化的完全匹配）。
	 -> 因此，您不能僅依賴指標名稱來確定發生了什麼，需要查看程式碼。
- **不同的正規化**。
	- 回到上面的 `exact match` 比較例子，在 `lm_eval` v1 中，許多任務被簡單地命名為生成式 `exact match`：您會從中假設預測結果*按原樣*與參考答案進行比較。
	  查看程式碼後，預測結果實際上會經過一個正規化步驟（移除標點符號、統一數字等）後再與參考答案比較。這顯然會大幅改變結果。
	  （`lm_eval` v2 現在在大多數指標名稱中包含了正規化名稱。）
	 -> 這是最容易出錯的事情之一，特別是對於需要大量正規化/答案後處理的任務，例如數學評估（您想從生成的解釋中提取答案）。

## 不同的提示
提示變化主要有 3 個因素。
### 提示本身
您使用的提示格式可以且會大幅改變分數。

例如，對於多選題答案，在呈現選項時的一些常見格式包括非常簡單的變化，例如：
```
Question: <text of the question>
Choices:
```
```markdown
| A. <Choice A> | (A) <Choice A> | <Choice A> |
| B. <Choice B> | (B) <Choice B> | <Choice B> |
| C. <Choice C> | (C) <Choice C> | <Choice C> |
| D. <Choice D> | (D) <Choice D> | <Choice D> |
```
```
Answer:
```
並預測 `A`/`B`/`C`/`D` 或 `<Choice A/B/C/D>`。

這些提示在**語意上是等價的**，因為它們包含完全相同的內容——但它們仍然可能導致*同一模型出現幾個百分點的差異*。我們在[這裡](https://x.com/clefourrier/status/1777319187913875893/photo/1)做了一些實驗（您會看到同一模型有高達 7 個百分點的差異），而[一篇論文也觀察到類似的結果](https://arxiv.org/abs/2310.11324)。

某些任務還會加上任務提示前綴（例如：`The following questions are about <topic>`）——其存在與否也會影響分數。

[這篇出色的論文](https://arxiv.org/abs/2407.07890)⭐ 也突顯了這一點的副作用：許多模型現在被訓練過度擬合基準測試的提示和答案格式，犧牲了在評估時適應其他提示的能力。

這是我們在 Open LLM Leaderboard 2 上對 Llama3.1 模型觀察到的現象。它們對我們的 MATH-Hard 評估預測出了正確答案，但得分很低，因為它們無法適應 few-shot 中提供的模板，而過度擬合了 GSM8K 提示和答案格式（另一個數學評估）。
### 系統提示和對話模板
對話模型通常經過指令/偏好訓練或微調。在此階段，它們學會在推論時遵循特定的模板。例如，模板可能需要以一般提示（稱為 `system prompt`）開始對話輪次，並以特定標記（通常是 `System: `）為前綴。該提示用於為模型提供高階指令，例如角色的內容或一般的回答風格指令。對話輪次還可能需要在文本前添加前綴關鍵字，例如查詢的 `User` 和答案的 `Assistant`。

在使用 few shot 時，您還需要選擇是否希望範例以多輪方式提供（模擬使用者/助理輪次）或一次性提供（在單個使用者提示中）。

在推論時不遵循模型期望的對話模板會嚴重影響其性能，因為它會將輸出驅動到模型一直在收斂的機率空間之外。

### Few-shot 樣本
Few-shot 樣本有兩件事很容易出錯（如果您不確定它是什麼，請參閱 `general-knowledge/Model inference`）。

顯然，您需要使用與參考任務**相同數量的 few-shot 樣本**。

然而，您還需要使用與您正在比較的模型**完全相同的樣本**，因為使用不同的樣本會改變結果（如果我們假設某些樣本比其他樣本更能表達任務，這並不令人驚訝）。也許更令人驚訝的是：您不僅需要使用完全相同的樣本，還需要以**完全相同的順序**呈現它們。在相同樣本上變化順序導致我們在 MMLU 的某些子集上觀察到高達 3 個百分點的差異（您可以在[這裡看到一些結果](https://huggingface.co/blog/evaluation-structured-outputs)，這是第三個色彩網格）。

這也是需要注意隨機種子的地方。

## 不同的生成參數
對於生成式評估，需要注意的參數包括：
- 確保您使用**相同的句子結束標記**
- 確保您允許模型在評估中**生成相同數量的標記**
- 確保在使用取樣時，您使用**相同的種子/溫度參數**

## 不同的模型載入方式
我們觀察到的一些差異來源包括：
- 使用**不同的硬體**。
  Pytorch 不保證非確定性操作在不同硬體上的可重現性
- 使用**不同的函式庫**。
  例如，如果您使用 `transformers` 與 `vllm` 作為推論的後端，矩陣計算的管理方式並不完全相同
- 使用**不同的批次大小**。
  在多個評估函式庫和模型後端中都有記錄，使用不同的批次大小會改變推論結果——如果您想要完全可重現的評估，應該固定批次大小，儘管出於記憶體問題，這可能並不總是可行
- 為您的模型權重使用**不同的載入精度**。
  使用較低的精度可以減少記憶體和推論成本，但也會改變數值結果，因為您使用的是不同版本的權重。
