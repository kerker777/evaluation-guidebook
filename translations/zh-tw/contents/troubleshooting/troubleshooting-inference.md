# 推論問題排除

## 我的模型跑得很慢！
### 調整批次大小
如果您想要絕對的可重現性（在特定硬體和特定評估提示下），您可能會使用批次大小為 1。然而，改用較大的批次大小可能會讓評估速度更快（前提是它符合您硬體的記憶體需求）

### 資料平行化
您也可以在多個 GPU 上複製您的模型，而不是只載入到單一 GPU，並將資料子集提供給每個 GPU 副本，然後彙整運算結果。
這表示每個資料流都會與其他資料流同時平行處理，從而將您的總執行時間除以 GPU 數量。
不過，如果可以的話，所有 GPU 都應該在單一節點上，以避免節點間的瓶頸。

### 修改推論程式碼
並非所有推論函式庫的執行速度都相同，有些程式碼比其他程式碼更優化。您需要稍微實驗一下，找出哪些函式庫的推論速度最快，如果您使用 pytorch，我建議參考[這裡](https://pytorch.org/serve/performance_checklist.html)的模型推論優化檢查清單。

### 調整精度
如果您的模型非常慢，您可以透過降低運算精度來減少其大小。以 float32 儲存的模型會進行非常精確的運算（每個儲存的數字使用 32 位元！），這也非常耗費記憶體和運算資源 - 改用 `bfloat16` 或 `float16`（一半的精度）應該能讓模型速度快兩倍，精度損失應該幾乎不會造成影響。如果您想要更大幅度的速度提升，您可以將其量化到更低，到 8 或 4 位元（例如使用 `gptq` 或 `bitsandbytes`），因為 n 位元矩陣運算應該會更快，而且您的模型在記憶體中佔用的空間會更小（不過，某些量化函式庫可能會有點慢，所以請針對您的使用情境進行測試！）。

## 我的模型太大了！
### 估算記憶體需求
您可以使用**以下公式**來估算載入特定模型（以及因此所需的硬體）所需的最小理論記憶體：

`<記憶體（單位 GB）> = <參數數量（單位 G）> * <精度係數>`

由於您可以在一個位元組中儲存 8 位元，所需的記憶體就是參數總數乘以儲存一個參數所需的位元組數。因此，`float32` 的精度係數為 4，`float16` 或 `bfloat16` 為 2，`8bit` 為 1，`4bit` 模型為 0.5，依此類推。

就是這樣！

為了更保險起見，我實際上會建議使用 `<記憶體（單位 GB）> = <參數數量（單位 G）> * (<精度係數> * 110%)`，因為推論所需的記憶體會比僅載入模型多一些（您還需要載入批次資料）。

### 如果您的模型無法放入 GPU 該怎麼辦？
#### 量化
第一個顯而易見的方法是調整上述的 `<精度係數>`：從 float32 降到 4 位元可以將記憶體需求減少 8 倍！
然而，使用過低的精度可能會產生較差的結果，因此對於某些模型（特別是中等規模的模型），您可能會想要保持在 float16 或 8bit。（量化似乎對非常大的模型效能影響較小，可能是因為資訊冗餘的關係）。
#### 模型平行化
模型平行化包含一系列技術，將您的模型切割成較小的子模型片段，在單一不同的 GPU 上載入並執行這些較小的片段。這需要較少的記憶體，因為您永遠不會一次載入完整的模型，但可能會比較慢。

兩種主要的模型平行化類型是
- 流水線平行化（Pipeline parallelism），將模型在整個層級分割，並將各層分配到不同的 GPU 上。由於第 1 層的輸出是第 2 層的輸入，這會導致執行速度較慢，因為 GPU 在等待時會閒置，這稱為「氣泡」（bubble）（而且資料必須從一個 GPU 傳輸到下一個 GPU）。可以透過將輸入分割成較小的批次來減少氣泡。這正在透過 `PiPPy` [函式庫](https://github.com/pytorch/PiPPy)原生加入 PyTorch，這也是 `accelerate` 在底層用於平行化的方式。
- 張量平行化（Tensor parallelism），將模型在矩陣運算層級分割。這表示矩陣將按行或列分割，並彙整總結果。只要所有 GPU 都在同一個節點上（以避免節點間網路瓶頸），這非常有效率，但可能難以編寫程式碼。您可以在 `vllm` 函式庫中找到很棒的實作。它提供**驚人的速度提升**。

關於不同類型平行化（包括資料平行化，用於加速）的最佳文件在[這裡](https://huggingface.co/docs/transformers/v4.15.0/en/parallelism)。

#### CPU 卸載
CPU 卸載將一些運算和模型部分移至 CPU，以減少 GPU 記憶體使用量。它比這裡的任何其他方法都**慢得多**，主要是因為您需要一直在不同裝置之間移動資料。

一個例子是 Deepspeed 的 [ZeRO-Offload](https://arxiv.org/abs/2101.06840)，它在 CPU 和 GPU 之間分配參數（除了使用 ZeRO-2 論文中描述的其他優化）。傳遞到 CPU 的是梯度、優化器狀態和優化期間的 fp32 模型參數運算，而在 GPU 上，您會找到 fp16 參數和前向/後向傳遞，以利用 CPU 記憶體使用和 GPU 運算，同時最小化兩者之間的通訊。

### 我的模型可以放入 GPU，但我仍然遇到記憶體不足（OOM）！
那麼您可能在上下文大小方面遇到問題。

我們建議：
1) 測試您的模型是否真的可以在載入一些虛擬推論資料的情況下放入 GPU。這些虛擬推論資料應該要有足夠大的上下文大小（能代表您的任務）
2) 降低批次大小，或移除可能導致意外 OOM 錯誤的自動批次大小搜尋（如果您有啟用此功能）
3) 更廣泛地說，確保樣本以上下文大小的反向順序呈現給您的模型，以確保如果上下文大小太大，您的模型會立即失敗，而不是在執行 X 小時後才失敗。
