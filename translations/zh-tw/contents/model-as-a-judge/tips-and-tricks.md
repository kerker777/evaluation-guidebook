# 技巧與訣竅

## 減緩 LLM 作為評審的已知偏誤：

- **缺乏內部一致性**：如果您多次提示評審模型（當溫度參數不為 0 時），它可能會給出不同的判斷結果
	- 您可以透過對評審模型進行自我一致性提示來減緩此問題，多次提示並保留多數輸出結果
- **自我偏好**：在評分答案時，它們傾向於[偏好自己的輸出](https://arxiv.org/abs/2404.13076)
	- 您可以透過使用評審團來減緩此問題
- **對輸入干擾的盲目性**：模型不擅長識別[受干擾的輸入](https://arxiv.org/abs/2406.13439)，並且相關地[不擅長提供一致的評分範圍](https://twitter.com/aparnadhinak/status/1748368364395721128)（更詳細的實驗請見[這裡](https://github.com/LeonEricsson/llmjudge/blob/main/README.md)）。例如，如果要求對已在一致尺度上添加雜訊的文本進行品質排名，預測的分數並不能反映此尺度。
	- 您可以透過以下方式減緩此問題：
		- 要求模型在[提供分數之前](https://twitter.com/seungonekim/status/1749289437165769177)先解釋其推理過程
		- 在提示中提供一致的評分標準。
- **位置偏誤**：它們傾向於[偏好特定答案位置](https://arxiv.org/abs/2306.05685)。例如，在呈現成對比較時，Claude 和 GPT3.5 往往會相當系統性地偏好第一個選擇，或第二個選擇
	- 您可以透過以下方式減緩此問題：
		- 隨機切換答案位置
		- 計算所有可能選擇的對數機率以獲得標準化答案
- **冗長偏誤**（或長度偏誤）：它們傾向於喜歡更冗長的答案
	- 您可以透過[考慮答案長度差異](https://arxiv.org/abs/2404.04475)來減緩此問題
- **與[人類答案](https://arxiv.org/abs/2308.15812)的一致性有爭議：**
	- 然而，[非專業人類是否適合作為所有評估的良好基準也是有爭議的](https://arxiv.org/abs/2202.06935)。對於某些特定領域（醫療、法律、數學等），依賴非專業的人類標註者作為基準，與直接使用 LLM 一樣不可靠。
- **格式偏誤**：如果提示格式[與訓練時的格式相差太遠](https://arxiv.org/abs/2310.17631)，它們往往無法準確評估。例如，一個訓練用於進行成對比較並附加參考答案的模型，如果未提供該答案就會失敗，反之亦然。
	- 您可以透過注意訓練提示格式（如果模型經過指令微調）並確保遵循該格式來減緩此問題。

## 為 LLM 評審選擇合適的任務

LLM 評估器：
- 整體而言**不擅長識別幻覺**，特別是所謂的部分幻覺（看起來接近真實答案但實際上略有不同）（請參見[這裡](https://arxiv.org/abs/2305.11747)和[這裡](https://arxiv.org/abs/2303.08896)）
- 在[摘要](https://arxiv.org/abs/2304.02554)（[這裡也是](https://arxiv.org/abs/2303.16634)）、[忠實度](https://arxiv.org/abs/2307.16877)方面與人類標註者的相關性較低到尚可，並且在更廣泛的[任務範圍](https://arxiv.org/abs/2406.18403)上與人類判斷的相關性並不一致
