# 獎勵模型呢？

## 什麼是獎勵模型？

獎勵模型會學習從人類標註中預測給定提示詞/完成回應配對的分數。最終目標是讓它們做出與人類偏好一致的預測。
一旦訓練完成，這些模型就可以作為人類判斷的代理獎勵函數，用來改進其他模型。

### 成對評分

最常見的獎勵模型類型是 Bradley-Terry 模型，它會輸出單一分數，遵循以下公式：

$$p(\text{completion b is better than completion a}) = \text{sigmoid}(\text{score}_b - \text{score}_a)$$

這個模型僅使用完成回應的成對比較來訓練，這比收集分數更容易，但只能比較單一提示詞的多個完成回應，而無法跨提示詞比較完成回應。

其他模型則擴展了這種方法，預測一個更細緻的機率來判斷某個完成回應優於另一個（[範例](https://huggingface.co/RLHFlow/pair-preference-model-LLaMA3-8B)）。

這使得它們（理論上）能夠判斷完成回應之間的細微差異，但代價是無法輕鬆儲存和比較同一測試集中跨提示詞的多個不同分數。此外，在比較過長的完成回應時，上下文長度和記憶體限制可能會成為問題。

### 絕對評分

某些獎勵模型（如 [SteerLM](https://arxiv.org/abs/2311.09528)）會輸出絕對分數，可以直接用於評估完成回應，無需進行成對比較。這些模型在評估時可能更容易使用，但收集資料也更困難，因為在人類偏好中，絕對分數往往比成對評分更不穩定。

最近，有人提出了同時輸出絕對和相對分數的模型，例如 [HelpSteer2-Preference](https://arxiv.org/abs/2410.01257) 和 [ArmoRM](https://arxiv.org/abs/2406.12845)。


## 如何使用獎勵模型進行評估？

給定一個提示詞資料集，我們可以從語言模型生成完成回應，並要求獎勵模型為其評分。

對於提供絕對分數的模型，可以對結果分數取平均值以獲得合理的總結分數。

然而，在更常見的相對評分情況下，平均獎勵可能會受到異常值（少數非常好或非常差的完成回應）的影響而產生偏差，因為不同的提示詞可能本質上具有不同的獎勵尺度（某些提示詞比其他提示詞難得多或容易得多）。

相反地，我們可以使用：
- 勝率：採用一組參考完成回應，並計算模型完成回應中排名高於參考完成回應的百分比。這種方法稍微更細緻。
- 勝出機率：完成回應優於參考完成回應的平均機率，可以提供更細緻且平滑變化的訊號。

## 獎勵模型的優缺點

獎勵模型通常具有以下特點：
- **非常快速**：獲得分數就像執行一次相對較小模型的前向傳遞一樣簡單（因為我們只獲得分數，而不是長文本，這與評判 LLM 相反）
- **確定性**：透過相同的前向傳遞會產生相同的分數
- **不太可能受到位置偏差影響**：由於大多數模型只接受一個完成回應，因此不會受到順序的影響。對於成對模型，只要訓練資料在包含第一個和第二個答案作為最佳答案方面是平衡的，位置偏差通常也很小。
- **無需提示詞工程**：因為模型會根據其訓練的偏好資料，直接從一個或兩個完成回應中輸出分數。

另一方面，它們：
- **需要特定的微調**：這可能是一個相對昂貴的步驟，儘管它們從基礎模型繼承了許多能力，但在訓練分佈之外的任務上仍可能表現不佳。
- **在強化學習和評估中同時使用時會降低效率**（或在與獎勵模型訓練資料相似的資料集上使用直接對齊演算法時），因為語言模型可能會過度擬合獎勵模型的偏好。

## 使用獎勵模型進行評估的技巧與訣竅

- 尋找高性能模型的好地方是 [RewardBench 排行榜](https://huggingface.co/spaces/allenai/reward-bench)。
- 您可以查看 [Nemotron](https://arxiv.org/abs/2406.11704) 論文中如何使用獎勵模型。
- 對於評估單一提示詞和完成回應的獎勵模型，您可以快取許多參考模型的分數，並輕鬆查看新模型的表現。
- 在訓練過程中追蹤勝率或機率，例如在[這篇](https://arxiv.org/abs/2410.11677v1)最近的論文中，可以讓您偵測模型退化並選擇最佳檢查點。
