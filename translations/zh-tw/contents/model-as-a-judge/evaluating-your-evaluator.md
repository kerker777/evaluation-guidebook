# 評估您的評估器

在正式環境或大規模使用評判 LLM 之前，您會希望先針對您的任務評估其品質，以確保它的評分對您來說確實相關且有用。

注意：*如果評估器預測二元輸出，這會更容易執行，因為您可以使用可解釋的分類指標（準確率/召回率/精確率）。如果它預測的是量表分數，則很難估計與參考標準之間的相關性品質。*

因此，一旦您選定了模型評判器及其提示詞，您需要執行以下步驟。

## 1. 選擇您的基準
您需要將評估器的判斷與基準進行比較：基準可以是人工標註、您已知在該任務上表現優異的另一個評判模型的輸出、黃金標準、使用不同提示詞的同一模型等。

您不一定需要大量範例（50 個可能就足夠），但它們需要極具代表性，能夠區分任務的特徵（特別是邊界案例的代表性），並且品質盡可能高。

## 2. 選擇您的指標
您的指標將用於比較評判器的評估結果與參考標準。

一般來說，如果您的模型預測的是二元分類或進行成對比較，這種比較會容易得多，因為您可以計算準確率（用於成對比較）或精確率和召回率（用於二元分類），這些都是非常容易解讀的指標。

比較分數與人工或模型評分的相關性會更困難。若要更詳細地了解原因，我建議您閱讀這篇精彩的[部落格相關章節](https://eugeneyan.com/writing/llm-evaluators/#key-considerations-before-adopting-an-llm-evaluator)。

一般來說，如果您對於何時該選擇什麼指標（就模型、指標等而言）感到迷茫，您也可以參考[同一部落格](https://eugeneyan.com/writing/llm-evaluators/) ⭐ 中的[這張有趣的圖表](https://eugeneyan.com/assets/llm-eval-tree.jpg)。

## 3. 評估您的評估器
在這個步驟中，您只需使用您的模型及其提示詞來評估測試樣本！然後，一旦獲得評估結果，使用上述的指標和參考標準來計算評估的分數。

您需要決定接受的門檻是什麼。根據任務的難度，如果您進行的是成對比較，可以設定 80% 到 95% 的準確率為目標。關於相關性（如果您使用分數），文獻中的研究者通常對與參考標準達到 0.8 的 Pearson 相關係數感到滿意。不過，我也看過一些論文宣稱 0.3 就表示與人工標註者有良好的相關性（^^"），所以這可能因情況而異。


