# 設計你的評估提示詞

## 通用提示詞設計技巧
在設計提示詞本身時，我從網路上蒐集到一些通用的指導原則：
- 提供清楚的任務描述：
	- `你的任務是執行 X`。
	- `你將會收到 Y`。
- 提供明確的評估標準說明，必要時包含詳細的評分系統：
	- `你應該以 1 到 5 的量表評估屬性 Z，其中 1 表示...`
	- `你應該評估樣本 Y 中是否存在屬性 Z。當...時，屬性 Z 即存在`
- 提供一些額外的「推理」評估步驟：
	- `要評判這個任務，你必須先仔細閱讀樣本 Y 以識別...，然後...`
- 指定期望的輸出格式（增加欄位將有助於一致性）
	- `你的答案應以 JSON 格式提供，格式如下 {"Score": 你的分數, "Reasoning": 讓你得出此分數的推理過程}`

你可以也應該從 [MixEval](https://github.com/huggingface/lighteval/blob/main/src/lighteval/tasks/extended/mix_eval/judge_prompts.pyy) 或 [MTBench](https://github.com/huggingface/lighteval/blob/main/src/lighteval/tasks/extended/mt_bench/judge_prompt_templates.py) 的提示詞模板中獲取靈感。

其他要點：
- [成對比較與人類偏好的相關性更高](https://arxiv.org/abs/2403.16950)，相較於評分方式，通常也更為穩健。
- 如果你真的需要分數，請使用整數量表，並確保為[每個分數的含義](https://x.com/seungonekim/status/1749289437165769177)提供詳細說明，或使用累加式提示詞（`若答案具有此特徵則給 1 分，若...則再加 1 分` 等等）
- 針對每項能力使用獨立的提示詞進行評分，往往能獲得更好且更穩健的結果

## 提升評判準確度
你也可以使用以下可能成本較高的技術來提升準確度：
- **少樣本範例**：如同許多其他任務，如果你提供範例，可以幫助模型的推理。然而，這會增加你的上下文長度。
- **參考資料**：如果有參考資料，你也可以用它來增強提示詞，這能提升準確度
- **思維鏈（CoT）**：[提升準確度](https://arxiv.org/abs/2212.08073)，如果你要求模型在給出分數**之前**先輸出其思維鏈（[這裡](https://x.com/seungonekim/status/1749289437165769177)也有觀察到）
- **多輪分析**：可以改善[事實錯誤偵測](https://arxiv.org/abs/2305.13281)
- 使用**評審團**（多個評審，你從中選取答案的彙總結果）：相較於使用單一模型，[能獲得更好的結果](https://arxiv.org/abs/2404.18796)。
	- 透過使用多個較小的模型而非一個大型昂貴的模型，可以大幅降低成本。
	- 你也可以嘗試使用單一模型但變化溫度參數
- 令人驚訝的是，社群發現在提示詞中增加「賭注」（`正確回答就能得到一隻小貓`）可以提升正確性。這個方法的效果可能因人而異，請依你的需求調整。

關於提示詞的注意事項：根據你的使用情境的重要程度，為了盡可能減少偏差，你會想要參考社會學領域在如何設計良好問卷方面所做的研究。如果你將評估器視為人類標註者的替代品，那麼你需要關注類似的指標：計算標註者間一致性、使用正確的調查設計方法論來減輕偏差等等。

然而，大多數人並不真的需要可重現且高品質、無偏差的評估，透過還算可以的提示詞進行快速而粗略的評估就已經很滿意了。（這也是可以接受的情況！只取決於所附帶的後果）。
