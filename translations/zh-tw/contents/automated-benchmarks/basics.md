# 基礎概念

*註：本文內容與我撰寫的 [通用評估部落格](https://huggingface.co/blog/clefourrier/llm-evaluation) 有部分重疊*
## 什麼是自動化基準測試？

自動化基準測試通常以以下方式運作：你想知道你的模型在某些方面的表現如何。這個「某些方面」可以是一個定義明確的具體**任務**，例如「我的模型在垃圾郵件分類上的表現如何？」，也可以是一個更抽象且通用的**能力**，例如「我的模型的數學能力有多好？」。

基於此，你可以使用以下元素來建構評估：
- 一個**資料集**，由多個**樣本**組成。
	- 這些樣本包含模型的輸入，有時會搭配一個參考答案（稱為 gold），用來與模型的輸出進行比較。
	- 樣本的設計通常是為了盡可能模擬你想測試模型的情境：例如，如果你在研究電子郵件分類，你會建立一個包含垃圾郵件和非垃圾郵件的資料集，並試著加入一些具挑戰性的邊界案例等。
- 一個**評估指標**。
	- 評估指標是為你的模型評分的方式。
	  範例：你的模型分類垃圾郵件的準確度如何（正確分類的樣本得分為 1，錯誤分類為 0）。
	- 評估指標使用你的模型輸出來進行評分。在大型語言模型（LLM）的情況下，人們主要考慮兩種輸出：
		- 模型根據輸入所生成的文字（*生成式評估*）
		- 提供給模型的一個或多個序列的對數機率（*多選題評估*，有時稱為 MCQA，或*困惑度評估*）
		- 關於這方面的更多資訊，你應該查看[模型推論與評估](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/general-knowledge/model-inference-and-evaluation.md)頁面。

在模型從未接觸過的資料（訓練集中不存在的資料）上進行評估會更有意義，因為你想測試模型是否能良好地**泛化**。例如，在只見過假冒銀行垃圾郵件的情況下，模型能否分類與「健康」產品相關的垃圾郵件。

註：*只能在訓練資料上預測良好（且沒有潛在地學習到更高層次通用模式）的模型被稱為**過度擬合**。這類似於學生死記硬背考題卻不理解主題，在已經出現在訓練集中的資料上評估 LLM，等於是在評分它們實際上不具備的能力。*

## 使用自動化基準測試的優缺點
自動化基準測試具有以下優點：
- **一致性與可重現性**：你可以在同一個模型上執行相同的自動化基準測試 10 次，會得到相同的結果（除非受到硬體變化或模型固有隨機性的影響）。這意味著你可以輕鬆地為特定任務建立公平的模型排名。
- **有限成本下的規模化**：目前這是評估模型最經濟實惠的方式之一。
- **易於理解**：大多數自動化評估指標都非常容易理解。
  *例如：完全匹配會告訴你生成的文字是否與參考答案完全相符，而準確率會告訴你有多少案例中選擇的答案是正確的（不過對於 `BLEU` 或 `ROUGE` 等指標來說，理解難度會稍微高一些）。*
- **資料集品質**：許多自動化基準測試使用專家生成的資料集或既有的高品質資料（如 MMLU 或 MATH）。然而，這並不代表這些資料集是完美的：在 MMLU 中，後續發現了一些樣本的錯誤，從解析問題到實際上不合理的問題都有，這促使了幾個後續資料集的產生，如 MMLU-Pro 和 MMLU-Redux。

然而，它們也存在以下限制：
- **在更複雜任務上的應用受限**：自動化基準測試在表現容易定義和評估的任務（例如分類）上運作良好。另一方面，更複雜的能力較難分解為定義明確且精確的任務。
  *例如：「擅長數學」是什麼意思？是擅長算術？還是邏輯？還是能夠推理新的數學概念？*
  這導致了使用更**通用**的評估方式，不再將能力分解為子任務，而是假設整體表現將是我們想要測量目標的**良好代理**。
- **污染問題**：一旦資料集以純文字形式公開發布，它最終會出現在模型訓練資料集中。這意味著在為模型評分時，你無法保證它之前沒有看過評估資料。
