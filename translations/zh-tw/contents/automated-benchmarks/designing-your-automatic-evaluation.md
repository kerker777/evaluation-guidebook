# 設計您的自動化評測

## 選擇資料集
在進行評測時，您可以選擇使用現有的資料集（範例請參考[一些評測資料集](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/automated-benchmarks/some-evaluation-datasets.md)）或自行設計。在這個過程中，務必記住一點非常重要：**您的評測結果只會和您的評測資料集一樣好**。

### 選擇現有資料集
您必須仔細檢視資料集的各個組成部分。
#### 建立流程
- **實際樣本是由誰建立的？**
在我看來，專家建立的資料集 > 付費標註者資料集 ~ 群眾外包資料集 > MTurk 資料集。
您也要查看資料卡，在那裡可以找到標註者的人口統計資料 - 這對於理解資料集的語言多樣性很重要。

- **這些樣本是否都經過其他標註者或作者的審查？**
您需要了解：
	- 樣本的標註者間信度是否高（= 標註者的意見是否一致？）
	- 以及/或整個資料集是否經過作者審查。
這對於使用報酬偏低的標註者協助建立的資料集特別重要，因為這些標註者通常不是您目標語言的母語使用者（例如 AWS Mechanical Turk），否則您可能會發現拼字錯誤、文法錯誤或不合理的答案。

- **標註者是否獲得明確的資料建立指南？**
換句話說，您的資料集是否具有一致性？

#### 樣本
隨機抽取 50 個樣本並手動檢查：
- *品質方面*：
	- 提示詞是否清晰且不模稜兩可？
	- 答案是否正確？（*例如：TriviaQA 每個問題包含多個正確答案（別名欄位），有時會相互衝突。*）
	- 是否缺少資訊？（*例如：MMLU 在許多問題中缺少參考示意圖。*）
- *與您的任務的相關性*：
	- 這些問題是否是您想要用來評測大型語言模型的那種問題？
	- 這些範例是否與您的使用案例相關？

您也需要知道資料集中有多少樣本（以確保結果具有統計顯著性 - 自動化基準測試通常至少需要 100 個樣本）。
### 設計您自己的資料集
設計自己的資料集時，您可以採用 3 種方式。
#### 彙整現有資料
您可以從不同來源彙整現有資料，評測與您任務相關的能力。例如，許多評測資料集是由人工評測資料集彙整而成（如 MATH、LSAT 等）。在這種情況下，請遵循上述步驟。
#### 使用人工標註者
在「人工評測」中有專門介紹如何使用人工標註者的章節，請參考[使用人工標註者](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/human-evaluation/using-human-annotators.md)。
#### 使用合成資料
- **使用大型語言模型**
關於這點，您可以查看由優秀的 HF 同事撰寫的非常棒的 [Cosmopedia](https://huggingface.co/blog/cosmopedia) 部落格文章！它主要研究如何建立合成訓練資料集，但類似的技術也可用於評測。
請務必事後手動檢查/篩選/檢視您的資料集（遵循上述步驟）。

- **使用基於規則的技術**
如果您的任務允許，這是獲得幾乎無限樣本供應並避免污染的非常好的方法！
範例可以參考 [NPHardEval](https://arxiv.org/abs/2312.14890)、[DyVal](https://arxiv.org/abs/2309.17167)、[MuSR](https://arxiv.org/abs/2310.16049)、[BabiQA](https://arxiv.org/abs/1502.05698) 等。

## 選擇推論方法
您需要選擇所需的推論方法類型。

使用對數機率（MCQA，多選題問答）非常適合多選題答案（通常用於測試模型知識或消歧能力）。
- 優點：
	- 確保所有模型都能獲得正確答案
	- 提供模型「信心度」（和校準）的代理指標
	- 評測速度快，特別是當我們要求模型只預測一個 token 時（A/B/C/D 選項索引，或 Yes/No 等）。
	- 能夠獲得小型模型任務表現的訊號
- 缺點：
	- 稍微高估了小型模型的表現，因為如果讓它們自由生成，它們可能會產生超出可用選項範圍的內容。
	- 某些模型[會根據選項呈現的順序偏好特定選項](https://arxiv.org/abs/2309.03882)，這可能導致評測結果不具代表性

使用生成式（QA，問答）非常適合您想測試流暢度、推理能力或模型實際回答問題能力的任何任務。
- 優點：
	- 應該實際上與大型語言模型生成流暢文字的能力相關，大多數時候這才是人們真正關心的
- 缺點：
	- 評分可能更困難（請參閱下面的「指標」章節）
	- 通常比對數似然評測稍微昂貴，特別是如果包含採樣的話

## 選擇提示詞
提示詞將決定：
- 提供給模型多少關於任務的資訊
- 如何向模型呈現這些資訊。

一般 MCQA 或 QA 的提示詞通常由以下部分組成：
- 任務提示（選用）：介紹您的任務。
- 上下文：為您的問題提供額外的背景資訊。
	- *例如：對於摘要或資訊擷取任務，您可以提供內容來源*
- 問題：提示詞的實際核心。
- 對於多選題評測，您可以加入選項
- 連接詞（`Question`、`Context`、`Choice`...）

定義提示詞時，您需要注意：
- 即使是語義上等效的提示詞的小變化，也可能使結果產生相當大的差異（請參閱[疑難排解：可重現性](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/troubleshooting/troubleshooting-reproducibility.md)中的「不同提示詞」章節），而且提示詞格式可能對特定模型有利或不利
	- 如何減輕這種情況：
		- 成本較高的方法是使用提示詞變體多次重新執行評測
		- 成本較低的方法是使用分配給難度相當的不同樣本的一系列提示詞格式執行一次評測
- 您可以為模型提供範例以幫助它遵循預期格式（使用少樣本範例），加入連接詞整體上也有幫助
- 但模型現在傾向於過度擬合特定的提示詞格式。
	- [這篇論文](https://arxiv.org/abs/2407.07890)在這個主題上非常出色，特別展示了某些模型如何因為過度擬合測試集**格式**而被高估
	- 在 Open LLM Leaderboard 2 上，我們特別觀察到 Llama 3.2 和 Qwen 2.5 由於這個原因，不再遵循少樣本設定中提供的提示詞格式。
- 對於許多指標，您需要非常受限的生成或輸出。
  *您可以在[模型推論與評測](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/general-knowledge/model-inference-and-evaluation.md)頁面的「限制模型輸出」章節中了解更多。*

## 選擇指標
如果您查看的是**對數機率**，您的指標會很簡單：您會想查看準確率（最有可能的選項是最佳選項的頻率）。按長度（字元、token 或 pmi）標準化很重要。您也可以查看困惑度、召回率或 f1 分數。

對於**生成式**評測，您的指標範圍會更廣。
您需要
1. 決定是按原樣比較生成結果，還是先用某種方式標準化它們。
	- 如果設計不當，標準化很容易[不公平](https://huggingface.co/blog/open-llm-leaderboard-drop)，但總體而言它們仍然在任務層級提供訊號。
	- 它們對於特定任務非常重要，例如數學評測，您可能想從格式化的輸出中提取結果。
	- 如果您想使用額外的機制（如思維鏈）進行評測以提高準確性，它們也會很重要，因為您需要從實際結果中移除推理軌跡
2. 決定如何將生成結果與參考進行比較。
   您可以使用從基於匹配的指標（完全匹配、前綴匹配等）到摘要和翻譯指標（ROUGE、BLEU、字元 n-gram 比較）的任何方法。關於現有指標的清單，您可以查看[這裡](https://github.com/huggingface/lighteval/wiki/Metric-List)，我稍後會新增一個章節說明何時使用哪種指標。

更一般地說，在選擇指標時，您需要記住您的任務真正關注的是什麼。對於某些領域（例如：醫療、與公眾互動的聊天機器人），您不想測量平均表現，而是需要一種方法來評測您將獲得的**最差表現**（在輸出的醫療品質、毒性等方面）。（*要深入了解，請查看這篇[部落格](https://ehudreiter.com/2024/07/10/challenges-in-evaluating-llms/)*）

## 智慧型新任務：功能測試如何？
在程式碼領域，您不僅要評測生成程式的語義，還要評測其實際功能。因此，一個好的方法是檢查為遵循提示詞而生成的程式碼是否正確通過為該任務設計的單元測試套件。

這種功能性方法極具前景，因為它
- 允許更輕鬆地生成測試案例（在許多情況下，您可以生成基於規則的測試案例）
- 因此減少過度擬合
- 在特定主動能力上測試模型

然而，這種方法需要創造力才能轉換為文字！

一個很好的例子是 IFEval，這是一個測試模型是否能遵循指令的評測基準。它的工作原理是建立一些格式指令（*新增這個數量的項目符號。只將一個句子大寫。*等），並嚴格測試格式是否被遵循。顯然需要更多工作來將這個想法擴展到其他要分析的文字特徵！
