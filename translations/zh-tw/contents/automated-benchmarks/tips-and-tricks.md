# 技巧與訣竅

## 管理資料污染
一般來說，您應該假設網際網路上公開的資料集已經或將會被污染。

減輕此問題的解決方案包括：
- 在評估集中提供**金絲雀字串**（如 [BigBench](https://github.com/google/BIG-bench) 所採用）：這是一種特定的字元組合，模型開發者可以在訓練集中搜尋此字串，以判斷是否包含評估資料
- 以**[加密](https://arxiv.org/abs/2309.16575)或[需授權](https://huggingface.co/datasets/Idavidrein/gpqa)**的形式提供評估集，使其無法輕易被網頁爬蟲解析，避免意外納入訓練集
- 執行[動態基準測試](https://arxiv.org/abs/2104.14337)：定期更新基準測試，讓模型無法「死記答案」（但這會增加資料集的成本）
- 如果您正在執行基準測試，可以嘗試[事後偵測污染](https://arxiv.org/abs/2311.06233)（例如，透過查看生成困惑度或設計對抗性版本的提示詞——但目前沒有任何方法能完全可靠地偵測污染）

然而，即使資料集被污染，在訓練過程中仍可能提供有價值的資訊與訊號。

## 您可能遇到的實務問題

### 微調模型、系統提示與對話模板
許多指令調整模型如果未能確保以下事項，可能會表現得非常糟糕：
- 在推論的最開始加入系統提示
- 使用對話模板來進行提示（通常會在對話輪次中加入 `Assistant` 和 `User` 前綴——更多相關資訊請參閱[這份精彩指南](https://huggingface.co/docs/transformers/main/en/chat_templating)）

同樣重要的是，不要假設不同的分詞器行為會相同，尤其是在對話模板方面。您可以從[這則推文](https://x.com/danielhanchen/status/1796952220619157694)中看到關於分詞空格與對話模板的精彩圖解。

![Spacing, tokenization and template](https://pbs.twimg.com/media/GPANfpiasAA9b6F?format=png&name=medium)

### 分詞

1. **將上下文與選項一起或分開進行分詞**

在進行多選題評估時，通常您會希望將上下文與選項一起分詞，因為這樣能產生對模型而言更自然且合理的標記序列。

然而，某些分詞器（如 [Llama 分詞器](https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257)）不滿足 `enc(上下文 + 選項) = enc(上下文) + enc(選項)` 的條件（會增加或移除空格）。這表示比較選項的對數機率並不容易，因為上下文的標記可能會「滲入」選項中，導致比較結果混亂。

因此，如果您的模型遇到這種情況，您可能需要分別計算上下文與選項的標記，然後在移除可能被加入的特殊句首/句尾標記後再串接它們。

2. **注意句首與句尾標記**

某些模型（如 `Gemma` 系列）對於推論時[句首標記的加入](https://github.com/EleutherAI/lm-evaluation-harness/pull/1465)極為敏感。您可能需要進行一些實驗來確認這種情況是否發生，並在評估時手動加入這些標記。

您也可能遇到模型未如預期在句尾標記處停止的問題（例如在 `\n` 處），因為您的模型不會單獨預測此標記，而是將其包含在更高層級的標記中（例如 `\n\n`，這可能是單一標記，尤其是在程式碼模型中）。在這種情況下，您可能需要加入特定的檢查來「回溯」生成的文字，確保在計算指標前於適當位置截斷生成的句子。

3. **多語言與分詞**

在進行多語言評估時，您還需要根據評估任務與指標來決定如何分詞。由於某些語言並非總是使用空格作為詞彙分隔符（例如韓文、泰文、日文、中文等），它們需要特定語言的分詞器才能正確切分，否則會影響諸如 [BLEU](https://github.com/EleutherAI/lm-evaluation-harness/issues/212)、F1 分數等指標的得分。

4. **程式碼評估與句尾標記**

程式碼模型通常將 `\n\t` 作為單一標記進行訓練。這表示在生成文字時，它們通常會在一步中生成 `\n\t`。如果某項任務將 `\n` 定義為句尾標記（= 停止生成），當模型將 `\n\t` 作為單一標記預測時，由於它與 `\n` 不同，模型會在 `\n\t` 後繼續生成。但實際上您仍希望模型停止。在這些情況下，您需要更新句尾標記，或定義一種機制來回溯最新標記的字元表示，以便事後停止（並截斷）生成。

### 多選題評估的簡易加速方法
如果您確保模型只需要為任務預測一個標記，就能大幅加速多選題的預測。

這樣一來，您不需要執行 `選項數量` 次預測（`上下文 + 選項 1`、`上下文 + 選項 2` 等），只需對 `上下文` 執行推論，並計算完整詞彙表（其中包含所有單標記選項）的機率分佈，即可在一次執行中取得感興趣的對數機率。

（這就是我們在 `lighteval` 中的做法）。

## 生成式評估結果異常不佳

首先應該做的永遠是詳細檢查模型的生成結果。在疑難排解時，一些常見的檢查項目包括：
- 模型輸出的解析過於嚴格（在計算指標前），導致答案遺失
    - 修正方法：調整您的解析邏輯
- 模型無法在少樣本情境下遵循您的輸出格式（在近期使用指令資料訓練的模型中很常見，如 llama 3.2 或 Qwen 2.5）
    - 修正方法：調整您的提示格式，或直接假設模型應該能在少樣本情境下遵循格式
- 模型過於冗長，從未給出正確答案（在長文本模型中更常見，我們在 Qwen 與 CommandR 模型中觀察到這種情況）
    - 修正方法：增加允許的上下文長度、在任務提示中加入簡潔指示，或直接假設模型應該能簡潔作答
