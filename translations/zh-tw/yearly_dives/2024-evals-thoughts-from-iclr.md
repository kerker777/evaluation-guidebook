(originally published at : https://huggingface.co/blog/clefourrier/llm-evaluation)

# 聊聊大型語言模型的評測

由於我的團隊在 Hugging Face 負責評測和排行榜相關工作，在兩週前的 ICLR 2024 會議上，有許多人想和我深入討論這個主題（這真的出乎我意料，非常感謝所有感興趣的朋友）。

透過這些討論，我發現有些我認為理所當然的評測概念，其實 1) 並不是廣為人知的想法 2) 似乎相當有趣。

所以讓我們把這些對話分享給更多人！

## 我們如何進行大型語言模型評測？

首先，讓我們對幾個定義達成共識。據我所知，目前主要有三種評測方式：自動化基準測試、人工評審，以及模型評審。每種方法都有其存在的理由、用途和限制。

### 基準測試

自動化基準測試通常是這樣運作的：你想知道模型在某件事上的表現如何。這件事可以是一個定義明確的具體**任務**，例如*我的模型能多好地區分垃圾郵件與正常郵件？*，或者是更抽象和廣泛的**能力**，例如*我的模型的數學能力有多好？*。

從這裡開始，你會建構一個評測，通常由兩個部分組成：
- 一組*樣本*，作為模型的輸入以觀察其輸出，有時會配上參考答案（稱為 gold）以供比較。樣本通常設計用來模擬你想測試模型的情境：例如，如果你要評估郵件分類，你會建立一個包含垃圾郵件和正常郵件的資料集，並試圖涵蓋一些困難的邊界案例等。對於大型語言模型，兩種主要任務是生成評估（將生成的文字與參考答案在標準化後進行比較），或多選題（比較提示詞之後各種可能續寫的相對對數機率）。
- 一個*指標*，用來計算模型的分數。例如，你的模型能多準確地分類垃圾郵件（分類正確的樣本得分 = 1，分類錯誤 = 0）。

在模型訓練集未包含的資料上進行測試更有意義，因為你想測試模型是否能良好**泛化**。你不會想要一個只能分類它已經「見過」的郵件的模型，那不會很有用！

註：一個只能在訓練資料上表現良好（且沒有潛在學習到更高層級的通用模式）的模型，我們稱之為**過擬合**。在不那麼極端的情況下，你仍然希望測試模型是否能夠泛化到訓練集分布之外的資料模式（例如，在只見過假銀行垃圾郵件後，能夠分類關於「健康」產品的垃圾郵件）。

這對於定義明確的任務效果相當好，因為性能「容易」評估和衡量：當你真的在測試模型的垃圾郵件分類能力時，你可以說「模型正確分類了這些樣本中的 n%」。對於大型語言模型基準測試，可能會出現一些問題，例如模型會[根據多選題選項的呈現順序而偏好特定選擇](https://arxiv.org/abs/2309.03882)，而生成式評估依賴的標準化方法如果[設計不當可能容易造成不公平](https://huggingface.co/blog/open-llm-leaderboard-drop)，但整體而言它們在任務層級上仍然提供了有效的訊號。

然而對於能力來說，很難將其分解為定義明確且精確的任務：「擅長數學」是什麼意思？擅長算術？擅長邏輯？能夠對數學概念進行推理？

在這種情況下，人們傾向於進行更「整體性」的評估，不將能力分解為實際任務，而是假設在通用樣本上的表現將是我們想要衡量目標的**良好代理指標**。例如，GSM8K 由實際的高中數學問題組成，需要一整套能力才能解決。這也意味著失敗和成功都很難解釋。某些能力或主題，例如「這個模型擅長寫詩嗎？」或「模型的輸出是否有幫助？」，用自動化指標評估起來更加困難——同時，模型現在似乎擁有越來越多的**通用**能力，因此我們需要以更廣泛的方式評估它們的能力。（例如，科學界曾爭論大型語言模型[能否畫出](https://arxiv.org/abs/2303.12712)獨角獸[或不能](https://twitter.com/DimitrisPapail/status/1719119242186871275)。目前很可能還不行，但顯然這是一個值得研究的重要問題。）

自動化基準測試還傾向於有另一個問題：一旦它們以純文字形式公開發布，就很可能（通常是意外地）出現在模型的訓練資料集中。一些基準測試創建者，如 BigBench 的作者，曾試圖透過添加「金絲雀字串」（一個非常特定的字元組合）來減輕這個問題，讓人們可以尋找並從訓練集中移除，但並非每個人都知道這個機制或試圖進行這種移除。而且基準測試的數量也不少，因此在資料中尋找所有基準測試的意外副本成本很高。其他選擇包括以[加密形式](https://arxiv.org/pdf/2309.16575)提供基準測試，或透過[限制存取系統](https://huggingface.co/datasets/Idavidrein/gpqa)提供。然而，在評估透過黑盒 API 提供的封閉式模型時，無法保證提供的資料不會在之後被內部用於訓練或微調。

評估資料集最終出現在訓練集中的情況稱為**污染**，被污染的模型將有很高的基準測試分數，但無法很好地泛化到底層任務（可以在[這裡](https://aclanthology.org/2023.findings-emnlp.722/)找到關於污染的詳細描述，這裡有一個有趣的[檢測方法](https://arxiv.org/abs/2311.06233)）。解決污染的一種方法是執行[**動態基準測試**](https://arxiv.org/abs/2104.14337)（定期更新資料集以在系統性的未見過新資料上提供分數的評估），但這種方法長期來看成本較高。

### 人工評審

解決污染和更開放式評估的一個方案是請人類評估模型輸出。

這通常透過讓人類先對模型下提示詞，然後根據指引對模型答案評分或對多個輸出進行排序來完成。使用人工評審允許研究更複雜的任務，比自動化指標更具靈活性。它也能防止大多數污染情況，因為書寫的提示詞（希望）是新的。最後，它與人類偏好有很好的相關性，因為這正是被評估的內容！

有不同的方法可以在人類參與的情況下評估模型。

**Vibes-checks**（氛圍檢查）是社群中一些成員個別進行的手動評估的名稱，通常在未公開的提示詞上進行，以獲得模型在許多使用案例上表現如何的整體「感覺」，範圍從程式碼撰寫到色情內容的品質。（我也看過「canary-testing」這個詞用於此，參考煤礦中的金絲雀這種高訊號的方法）。通常在 Twitter 和 Reddit 上分享，它們主要構成了軼事證據，並且往往對確認偏誤高度敏感（換句話說，人們傾向於找到他們在尋找的東西）。然而，一些人一直在嘗試進行更有系統的氛圍檢查評估；例如，用戶 *Wolfram Ravenwolf* 透過部落格以非常系統化的方式分享他的模型比較發現（參見[這裡](https://huggingface.co/blog/wolfram/llm-comparison-test-llama-3)的範例）。

使用社群回饋來建立大規模模型排名就是我們所說的**競技場**。一個著名的例子是 [LMSYS 聊天機器人競技場](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)，社群用戶被要求與模型聊天，直到他們發現一個比另一個更好。然後投票會被彙總為 Elo 排名（比賽排名）以選出「最佳」模型。這種方法的明顯問題是高度主觀性——很難從許多使用廣泛指引的社群成員那裡強制執行一致的評分，特別是因為評分者的偏好往往[受文化約束](https://arxiv.org/abs/2404.16019v1)（例如，不同的人偏好不同的討論主題）。人們可以希望透過投票的龐大規模，通過「群眾智慧」效應來平滑這種效應（這種效應是由統計學家 Galton 發現的，他觀察到試圖估計數值的個別答案，如豬的重量，可以被建模為以實際答案為中心的機率分布）。

最後一種方法是**系統化標註**，你向經過篩選的付費標註者提供極其具體的指引，以盡可能消除主觀性偏誤（這是 [ScaleAI](https://scale.com/guides/data-labeling-annotation-guide#hight-quality-data-annotations) 和大多數資料標註公司使用的方法）。然而，這可能會變得極其昂貴，因為你必須為每個想要評估的新模型持續以非自動化的方式進行評估，而且仍然可能受到人類偏誤的影響（這項[研究](https://arxiv.org/abs/2205.00501)顯示，具有不同身份的人往往對模型答案的毒性評級非常不同）。

最近的[研究](https://arxiv.org/pdf/2309.16349)也顯示，人類評估者往往基於第一印象而不是實際的事實性或忠實性來估計答案的品質。眾包標註者特別對語氣敏感，並低估了自信答案中事實或邏輯錯誤的數量。換句話說，如果一個模型以自信的語氣說錯誤的事情，人類評估者注意到的可能性要小得多，這可能會使評分偏向更自信的模型。（專家標註者較不容易受到這些偏誤的影響。）這種人類偏誤在另一篇[論文](https://arxiv.org/pdf/2310.13548)中得到證實：人類最有可能偏好符合他們觀點或與他們的意見或錯誤一致的答案，而不是事實正確的答案。

這些偏誤並不令人意外，但必須考慮在內：並非所有使用案例都應該依賴使用人工標註者，特別是眾包的、非專家的標註者——任何需要事實性的任務（例如程式碼撰寫、模型知識評估等）都應該包括另一種更穩健的評估類型來完成基準測試。

### 模型評審

為了降低人工標註者的成本，一些人研究使用模型或衍生產物（最好與人類偏好對齊）來評估模型的輸出。這種方法並不新鮮，你可以在 2019 年找到從[模型嵌入](https://arxiv.org/abs/1904.09675)衡量摘要品質的技術。

評分存在兩種方法：使用[通用高能力模型](https://arxiv.org/abs/2306.05685v4)或使用專門從偏好資料訓練的[小型專業模型](https://arxiv.org/pdf/2405.01535)進行辨別。前者的結果與人類偏好有良好的相關性，但大多數足夠強大的模型往往是封閉原始碼的，因此可能在 API 背後變化，且無法解釋。

大型語言模型作為評審有幾個強烈的限制：它們在評分答案時傾向於[偏好自己的輸出](https://arxiv.org/abs/2404.13076)，[無法提供一致的分數範圍](https://twitter.com/aparnadhinak/status/1748368364395721128)（儘管你可以透過要求模型在[提供分數之前解釋其推理](https://twitter.com/seungonekim/status/1749289437165769177)來改善這一點），並且實際上與[人類排名不太一致](https://arxiv.org/pdf/2308.15812)。

我個人對使用模型作為評審的主要不滿是，它們在答案選擇中引入了非常微妙且無法解釋的偏誤。我認為，就像在遺傳學研究中過度雜交會導致動物或植物功能失調一樣，透過使用大型語言模型來選擇和訓練大型語言模型，我們同樣可能引入微小的變化，這些變化將在幾代之後產生更大的影響。我相信這種類型的偏誤在較小且更專業的模型作為評審（例如毒性分類器）中較不可能發生，但這仍有待嚴格測試和證明。

## 我們為什麼要進行大型語言模型評測？

既然我們已經看到了如何進行評測，那它實際上有什麼用處呢？

我堅信人們進行評測主要有三個原因，這些原因往往被混為一談，但實際上**非常不同**，並且各自回答了一個獨立的問題。

### 1) 我的模型訓練得好嗎？我的訓練方法可靠嗎？——非迴歸測試

**非迴歸測試**是一個來自軟體產業的概念，用於確保小的變更沒有破壞整體方法。

其想法如下：當你向軟體添加新功能或修復程式碼庫中的問題時，你是否破壞了其他東西？這就是非迴歸測試的目的：確保你的軟體的預期高層級行為不會突然被（看似無關的）變更破壞。

當你選擇一個設置來訓練模型時，你想測試非常相似的東西，並確保你的變更（選擇不同的訓練資料、架構、參數等）沒有「破壞」這些屬性模型的預期性能。

舉個具體例子，你會期望一個 7B 基礎大型語言模型在訓練後在（多選題）MMLU 上得到 50 到 65 分，另一方面，知道在 20 到 30 之間波動的性能表明沒有發生學習。

對於「非迴歸」評估，你需要查看 1) 評估分數**軌跡**（現在的性能是否比開始訓練時更好），2) 評估分數**範圍**（性能是否在預期範圍內）。你實際上...不在乎精確的分數本身！

因此，這種評估並不是要告訴你關於實際模型能力的任何事情，而只是確認你的訓練方法與其他訓練方法「一樣可靠」，並且你的模型以類似的方式行為。我相信即使是一些簡單地查看文字困惑度（機率）變化的評估也足以完成這一步，但你通常希望基準測試具有高「訊號雜訊比」，或者換句話說，你希望確保分數的大幅變化反映了模型的重大轉變。

### 2) 哪個模型最好？我的模型比你的模型好嗎？——排行榜和排名

評測的下一個角色只是對模型進行排序，以找出並選擇整體上最好的架構和方法。如果你有一個排行榜，拿最好的模型，但它在你的使用案例上不起作用，那麼下一個最好的模型也不太可能起作用。在[他們的論文](https://arxiv.org/pdf/2404.02112)中關於從 ImageNet 時代的基準測試和資料集設計中吸取的教訓，作者認為，由於分數容易不穩定，評估模型的唯一穩健方法是透過排名，更具體地說，是透過找到提供一致且穩定排名的廣泛評估組。

我相信尋找排名穩定性確實是模型基準測試的一個極其有趣的方法，因為我們已經證明大型語言模型在自動化基準測試上的*分數*極易受到[提示詞的微小變化](https://huggingface.co/blog/evaluation-structured-outputs)的影響，而人工評估也不會更一致——而在使用穩健的評估方法時，*排名*實際上更穩定。

如果分數本身不那麼相關，那麼使用模型的相對順序能否告訴我們一些有價值的東西呢？

在 ICLR 2024 相關的評估全會上，Moritz Hardt 比較了對 Open LLM Leaderboard 添加擾動（透過微小的分數修改，完全在分數範圍內）以及對 Chatbot Arena 添加擾動（透過向競技場添加一個糟糕的競爭者，看它如何影響 Elo 排名）。目前這兩個基準測試都無法提供穩定且一致的排名。我們一定會在 Open LLM Leaderboard 的未來版本中探索這個方面！

### 3) 就模型能力而言，我們作為一個領域處於什麼位置？我的模型能做 X 嗎？

「你如何知道模型能否做 X？」是一個經常出現的問題，我認為這是一個非常有效的問題。

然而，對於任何複雜的能力，**我們目前不能只是說「這個模型在這方面是最好的」，而是「這個模型在這個任務上是最好的，我們希望它是這個能力的良好代理指標，但沒有任何保證」**。

我們非常缺乏關於機器學習模型能力的良好定義和框架，特別是關於推理和心智理論的能力。然而，這並非機器學習所特有！在人類和動物研究中，定義什麼構成一種「能力」也相當困難，而試圖提供精確分數的指標（例如 IQ 和 EQ）也是備受爭議和有爭議的，這是有道理的。

我們可能想要借鑑社會科學來思考能力評估，因為在這些領域，人們習慣認真思考資料收集和分析中的混淆因素。然而，我也相信很可能 1) 我們根本無法定義這些廣泛的能力，因為我們目前無法在人類和動物中定義它們，2) 為人類（或動物）設計的框架不會很好地轉移到模型上，因為底層行為和假設是不同的。

## 結論

大型語言模型評測目前以以下方式進行：
使用自動化基準測試，受到污染和缺乏「通用性」的影響（後者不一定是壞事，因為專業化評估是有趣的）
使用人工評估，往往在小規模上缺乏可重現性，整體上受到心理偏誤的影響（例如偏好諂媚的答案），儘管人們可以希望在大規模上一些偏誤會被平滑
使用模型作為評審，在評估時具有非常微妙的偏誤，可能不會被注意到但會在下游引入擾動。

然而，並非一切都失敗了：評測在其限制範圍內，仍然能夠提供一些訊號，說明哪些新的訓練方法或資料集看起來有前景或沒有，無論是從性能如何落在預期範圍內（非迴歸測試），還是從模型整體排名如何（使用足夠穩定的評估）來看。我們也可以希望透過結合足夠多的跨主題和任務的資料點，將為我們提供足夠的訊號來了解整體模型性能，但不會對更「通用」的能力做任何假設。

與炒作相反，我們目前無法真正評估「通用模型能力」，首先也是最重要的，因為我們沒有定義那是什麼意思。然而，大型語言模型評測作為一個研究領域，目前還處於起步階段，有很多工作要做，這非常令人興奮！可以從許多領域獲得靈感，從機器學習[可解釋性](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)到社會學，以定義新的指標和任務。跨學科工作可能會為該領域開闢非常新穎有趣的方向！

## 致謝
非常感謝在會議上對評測主題感興趣並討論的所有優秀人士，包括但不限於 Summer Yue（Scale AI）、Moritz Hardt（Max Planck Institute）、Luca Soldaini 和 Ian Magnusson（Allen AI）、Ludwig Schmidt（Anthropic）、Max Bartolo（Cohere）、Maxime Labonne（Liquid AI）、François Charton（Meta）、Alan Cooney（UK AI Safety Institute）和 Max Ryabinin（Together AI）。

同樣非常感謝 Hugging Face 的 Yacine Jernite 和 Irene Solaiman 對本文件提供的寶貴回饋。

最後但同樣重要的是，感謝 Hugging Face 的評測和排行榜團隊，特別是 Nathan Habib，感謝我們一起進行的討論和工作！
